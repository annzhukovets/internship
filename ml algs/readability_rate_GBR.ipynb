{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053d3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/annazhukovets/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## for data\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "import string \n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "## for sentiment\n",
    "from textblob import TextBlob\n",
    "## for ner, pos\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nltk.download('wordnet')\n",
    "## parameters searching\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "## rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "## pickle\n",
    "import dill as pickle\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14e7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "# a function  to create and save logs in the log files\n",
    "def log(path, file):\n",
    "    \"\"\"[Create a log file to record the experiment's logs]\n",
    "    \n",
    "    Arguments:\n",
    "        path {string} -- path to the directory\n",
    "        file {string} -- file name\n",
    "    \n",
    "    Returns:\n",
    "        [obj] -- [logger that record logs]\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the file exist\n",
    "    log_file = os.path.join(path, file)\n",
    "\n",
    "    if not os.path.isfile(log_file):\n",
    "        open(log_file, \"w+\").close()\n",
    "\n",
    "    console_logging_format = \"%(levelname)s %(message)s\"\n",
    "    file_logging_format = \"%(levelname)s: %(asctime)s: %(message)s\"\n",
    "\n",
    "    # configure logger\n",
    "    logging.basicConfig(level=logging.INFO, format=console_logging_format)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # create a file handler for output file\n",
    "    handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # set the logging level for log file\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # create a logging format\n",
    "    formatter = logging.Formatter(file_logging_format)\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1da9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "with open('pickles/X.pk', 'rb') as data:\n",
    "    X = pickle.load(data)\n",
    "\n",
    "# y\n",
    "with open('pickles/y.pk', 'rb') as data:\n",
    "    y = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c4c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log(path=\"logs/\", file=\"logs.csv\")\n",
    "\n",
    "# logger.info(\"Train GBR ('max_depth': 4, 'n_estimators': 150)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62584692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "class regressor_stratified_cv:\n",
    "    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n",
    "                 random_state = 0, strategy = 'quantile'):\n",
    "        self.group_count = group_count\n",
    "        self.strategy = strategy\n",
    "        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n",
    "                             random_state = random_state)\n",
    "        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n",
    "        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n",
    "                                            strategy = self.strategy)  \n",
    "            \n",
    "    def split(self, X, y, groups = None):\n",
    "        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n",
    "        return self.cv.split(X, kgroups, groups)\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups = None):\n",
    "        return self.cv.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ff8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class IDFVectorizer\n",
    "# to generate new feature with mean of idf\n",
    "class IDFVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        # removal of punctuation\n",
    "        PUNCT_TO_REMOVE = string.punctuation\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "        \n",
    "        # removal of stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        def remove_stopwords(text):\n",
    "            return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "        \n",
    "        # lemmatization \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatize_words(text):\n",
    "            return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        \n",
    "        # word frequency in docs\n",
    "        def doc_freq(word):\n",
    "            c = 0\n",
    "            try:\n",
    "                c = DF[word]\n",
    "            except:\n",
    "                pass\n",
    "            return c\n",
    "        \n",
    "        # idf vector generation\n",
    "        def mean_of_vector(tokens):\n",
    "            idf_vec = []\n",
    "            for token in np.unique(tokens):\n",
    "                df = doc_freq(token)\n",
    "                idf = np.log(N/(df + 1))\n",
    "                try:\n",
    "                    idf_vec.append(idf)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return np.mean(idf_vec)\n",
    "        \n",
    "        # lower casing\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt\"].str.lower()\n",
    "        # removal of punctuation\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_punctuation(text))\n",
    "        # removal of stopwords\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_stopwords(text))\n",
    "        # lemmatization \n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: lemmatize_words(text))    \n",
    "        # tokenizetion\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda x: [token for token in word_tokenize(x)])\n",
    "        \n",
    "        N = len(x_dataset[\"excerpt\"])\n",
    "        DF = {}\n",
    "        for i in range(N):\n",
    "            tokens = x_dataset[\"excerpt_proc\"].iloc[i]\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "            \n",
    "\n",
    "        for i in DF:\n",
    "            DF[i] = len(DF[i]) \n",
    "\n",
    "        x_dataset['idf_vec'] = x_dataset[\"excerpt_proc\"].apply(lambda x:  mean_of_vector(x))\n",
    "        \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb25e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing step\n",
    "# Drop the columns \n",
    "from sklearn.compose import ColumnTransformer\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['id', \n",
    "                                                                        'url_legal', \n",
    "                                                                        'license',\n",
    "                                                                        'excerpt',\n",
    "                                                                        'standard_error',\n",
    "                                                                        'ner_tags',\n",
    "                                                                        'pos_tags',\n",
    "                                                                        'excerpt_proc',\n",
    "                                                                        'nlp_text'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4ac80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('idf_vect', IDFVectorizer()),\n",
    "                 ('pre_processing',pre_process),\n",
    "                 ('gbr', GradientBoostingRegressor(max_depth = 2, n_estimators = 200))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e02ecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Train GBR w/o params\n",
      "INFO The rmse for GBR iteration 0: 0.863\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 1: 0.862\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 2: 0.822\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 3: 0.806\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 4: 0.841\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 5: 0.825\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 6: 0.840\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 7: 0.848\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 8: 0.859\n",
      "INFO -------------------------------\n",
      "INFO The rmse for GBR iteration 9: 0.855\n",
      "INFO -------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "n_repeats = 2\n",
    "group_count = 10\n",
    "cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "                           group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "\n",
    "\n",
    "logger.info(\"Train GBR w/o params\")\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predict = pipe.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, predict, squared=False)\n",
    "    logger.info(\"The rmse for GBR iteration {}: {:.3f}\".format(i, rmse))\n",
    "    logger.info(\"-------------------------------\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "044ef409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [50, 100, 150, 200]\n",
    "# max_depth = [2, 4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06bcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'gbr__n_estimators': n_estimators,\n",
    "# 'gbr__max_depth': max_depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d05724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(pipe, param_grid, scoring=\"neg_mean_absolute_error\", n_jobs=-1, cv=3,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5d7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58286675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c3d0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get importance\n",
    "# importance = pipe.steps[3][1].feature_importances_\n",
    "# # summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# # plot feature importance\n",
    "# pyplot.bar([x for x in range(len(importance))], importance)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51e7e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto', 'sqrt']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "# max_depth.append(None)\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [2, 5, 10]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'rrandom_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=pipe,\n",
    "#                                    param_distributions=random_grid,\n",
    "#                                    n_iter=5,\n",
    "#                                    scoring='neg_root_mean_squared_error',\n",
    "#                                    cv=3, \n",
    "#                                    verbose=1, \n",
    "#                                    random_state=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
