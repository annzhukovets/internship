{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exclusive-screen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:52:33.629195Z",
     "iopub.status.busy": "2021-06-15T06:52:33.628026Z",
     "iopub.status.idle": "2021-06-15T06:53:06.686603Z",
     "shell.execute_reply": "2021-06-15T06:53:06.687154Z"
    },
    "papermill": {
     "duration": 33.082686,
     "end_time": "2021-06-15T06:53:06.687511",
     "exception": false,
     "start_time": "2021-06-15T06:52:33.604825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## for data\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "import string \n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "## for sentiment\n",
    "from textblob import TextBlob\n",
    "## for ner, pos\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nltk.download('wordnet')\n",
    "## parameters searching\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "## rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "## pickle\n",
    "import dill as pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "criminal-yemen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:06.730397Z",
     "iopub.status.busy": "2021-06-15T06:53:06.729557Z",
     "iopub.status.idle": "2021-06-15T06:53:06.829718Z",
     "shell.execute_reply": "2021-06-15T06:53:06.829067Z"
    },
    "papermill": {
     "duration": 0.123967,
     "end_time": "2021-06-15T06:53:06.829918",
     "exception": false,
     "start_time": "2021-06-15T06:53:06.705951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developed-wellington",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:06.881864Z",
     "iopub.status.busy": "2021-06-15T06:53:06.880959Z",
     "iopub.status.idle": "2021-06-15T06:53:06.883853Z",
     "shell.execute_reply": "2021-06-15T06:53:06.883350Z"
    },
    "papermill": {
     "duration": 0.035395,
     "end_time": "2021-06-15T06:53:06.884010",
     "exception": false,
     "start_time": "2021-06-15T06:53:06.848615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['target', 'standard_error'], axis=1)\n",
    "# X = train[['excerpt']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removed-morgan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:06.930880Z",
     "iopub.status.busy": "2021-06-15T06:53:06.930101Z",
     "iopub.status.idle": "2021-06-15T06:53:06.933078Z",
     "shell.execute_reply": "2021-06-15T06:53:06.932425Z"
    },
    "papermill": {
     "duration": 0.03033,
     "end_time": "2021-06-15T06:53:06.933246",
     "exception": false,
     "start_time": "2021-06-15T06:53:06.902916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "class regressor_stratified_cv:\n",
    "    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n",
    "                 random_state = 0, strategy = 'quantile'):\n",
    "        self.group_count = group_count\n",
    "        self.strategy = strategy\n",
    "        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n",
    "                             random_state = random_state)\n",
    "        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n",
    "        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n",
    "                                            strategy = self.strategy)  \n",
    "            \n",
    "    def split(self, X, y, groups = None):\n",
    "        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n",
    "        return self.cv.split(X, kgroups, groups)\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups = None):\n",
    "        return self.cv.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "american-artist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:06.992812Z",
     "iopub.status.busy": "2021-06-15T06:53:06.992005Z",
     "iopub.status.idle": "2021-06-15T06:53:06.994514Z",
     "shell.execute_reply": "2021-06-15T06:53:06.994974Z"
    },
    "papermill": {
     "duration": 0.042351,
     "end_time": "2021-06-15T06:53:06.995189",
     "exception": false,
     "start_time": "2021-06-15T06:53:06.952838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the BaseEstimator\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# define the class OutletTypeEncoder\n",
    "# This will be our custom transformer that will create 3 new binary columns\n",
    "# custom transformer must have methods fit and transform\n",
    "class FeatureGenerator(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        # utils function to count the element of a list\n",
    "        def utils_lst_count(lst):\n",
    "            dic_counter = collections.Counter()\n",
    "            for x in lst:\n",
    "                dic_counter[x] += 1\n",
    "            dic_counter = collections.OrderedDict(\n",
    "                       sorted(dic_counter.items(),\n",
    "                       key=lambda x: x[1], reverse=True))\n",
    "            lst_count = [ {key:value} for key,value in dic_counter.items() ]\n",
    "            return lst_count\n",
    "        \n",
    "        # utils function create new column for each tag category\n",
    "        def utils_new_features(lst_dics_tuples, tag):\n",
    "            if len(lst_dics_tuples) > 0:\n",
    "                tag_type = []\n",
    "                for dic_tuples in lst_dics_tuples:\n",
    "                    for tuple in dic_tuples:\n",
    "                        type, n = tuple[1], dic_tuples[tuple]\n",
    "                        tag_type = tag_type + [type]*n\n",
    "                        dic_counter = collections.Counter()\n",
    "                        for x in tag_type:\n",
    "                            dic_counter[x] += 1\n",
    "                return dic_counter[tag]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "            \n",
    "        # num of words in excerpt\n",
    "        x_dataset['word_count'] = x_dataset[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "        # num of chars in excerpt\n",
    "        x_dataset['char_count'] = x_dataset[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "        # num of sentences in excerpt\n",
    "        x_dataset['sentence_count'] = x_dataset[\"excerpt\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "        # avg word len in excerpt\n",
    "        x_dataset['avg_word_length'] = x_dataset['char_count'] / x_dataset['word_count']\n",
    "        # avg sentence len in excerpt\n",
    "        x_dataset['avg_sentence_lenght'] = x_dataset['word_count'] / x_dataset['sentence_count']\n",
    "        # sentiment index of excerpt\n",
    "        x_dataset[\"sentiment\"] = x_dataset[\"excerpt\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "        x_dataset[\"nlp_text\"] = x_dataset[\"excerpt\"].apply(lambda x: nlp(x) )\n",
    "        # ner tag text and exctract tags into a list\n",
    "        x_dataset[\"ner_tags\"] = x_dataset[\"nlp_text\"].apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in x.ents] )\n",
    "        # count ner tags\n",
    "        x_dataset[\"ner_tags\"] = x_dataset[\"ner_tags\"].apply(lambda x: utils_lst_count(x))\n",
    "        # extract ner features\n",
    "        ner_tags_set = ['QUANTITY', 'MONEY', 'GPE',\n",
    "                    'NORP', 'CARDINAL', 'LOC',\n",
    "                    'ORDINAL', 'PRODUCT', 'FAC',\n",
    "                    'LANGUAGE', 'TIME', 'LAW',\n",
    "                    'EVENT', 'ORG', 'PERCENT',\n",
    "                    'WORK_OF_ART', 'PERSON', 'DATE']\n",
    "        for feature in ner_tags_set:\n",
    "            x_dataset[\"ner_tags_\" + feature] = x_dataset[\"ner_tags\"].apply(lambda x:\n",
    "                                                                 utils_new_features(x, feature))\n",
    "        \n",
    "        # pos tag text and exctract tags into a list\n",
    "        x_dataset[\"pos_tags\"] = x_dataset[\"nlp_text\"].apply(lambda x: [(token.text, token.tag_) \n",
    "                                for token in x] )\n",
    "        # count pos tags\n",
    "        x_dataset[\"pos_tags\"] = x_dataset[\"pos_tags\"].apply(lambda x: utils_lst_count(x))\n",
    "        # extract pos features\n",
    "        pos_tags_set = ['CC', 'POS', 'WDT', 'VBP', 'FW', ':', 'PRP$',\n",
    "                    'WRB', 'PRP', 'RP', 'RBS', 'NNP', 'CD', 'EX', 'PDT',\n",
    "                    'VBN', 'WP$', 'JJ', 'SYM', 'VBG', 'VB', 'JJS', 'VBD',\n",
    "                    'WP', ',', 'NNS', 'NN', 'VBZ', 'MD', 'RB', 'DT',\n",
    "                    'JJR', 'UH', 'NNPS', 'TO', 'RBR']\n",
    "    \n",
    "        for feature in pos_tags_set:\n",
    "            x_dataset[\"pos_tags_\" + feature] = x_dataset[\"pos_tags\"].apply(lambda x:\n",
    "                                                                 utils_new_features(x, feature))\n",
    "    \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "guilty-kentucky",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:07.050991Z",
     "iopub.status.busy": "2021-06-15T06:53:07.049955Z",
     "iopub.status.idle": "2021-06-15T06:53:07.053206Z",
     "shell.execute_reply": "2021-06-15T06:53:07.052660Z"
    },
    "papermill": {
     "duration": 0.039504,
     "end_time": "2021-06-15T06:53:07.053355",
     "exception": false,
     "start_time": "2021-06-15T06:53:07.013851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the class IDFVectorizer\n",
    "# to generate new feature with mean of idf\n",
    "class IDFVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        # removal of punctuation\n",
    "        PUNCT_TO_REMOVE = string.punctuation\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "        \n",
    "        # removal of stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        def remove_stopwords(text):\n",
    "            return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "        \n",
    "        # lemmatization \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatize_words(text):\n",
    "            return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        \n",
    "        # word frequency in docs\n",
    "        def doc_freq(word):\n",
    "            c = 0\n",
    "            try:\n",
    "                c = DF[word]\n",
    "            except:\n",
    "                pass\n",
    "            return c\n",
    "        \n",
    "        # idf vector generation\n",
    "        def mean_of_vector(tokens):\n",
    "            idf_vec = []\n",
    "            for token in np.unique(tokens):\n",
    "                df = doc_freq(token)\n",
    "                idf = np.log(N/(df + 1))\n",
    "                try:\n",
    "                    idf_vec.append(idf)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return np.mean(idf_vec)\n",
    "        \n",
    "        # lower casing\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt\"].str.lower()\n",
    "        # removal of punctuation\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_punctuation(text))\n",
    "        # removal of stopwords\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_stopwords(text))\n",
    "        # lemmatization \n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: lemmatize_words(text))    \n",
    "        # tokenizetion\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda x: [token for token in word_tokenize(x)])\n",
    "        \n",
    "        N = len(x_dataset[\"excerpt\"])\n",
    "        DF = {}\n",
    "        for i in range(N):\n",
    "            tokens = x_dataset[\"excerpt_proc\"].iloc[i]\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "            \n",
    "\n",
    "        for i in DF:\n",
    "            DF[i] = len(DF[i]) \n",
    "\n",
    "        x_dataset['idf_vec'] = x_dataset[\"excerpt_proc\"].apply(lambda x:  mean_of_vector(x))\n",
    "        \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regional-seating",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:07.096731Z",
     "iopub.status.busy": "2021-06-15T06:53:07.095982Z",
     "iopub.status.idle": "2021-06-15T06:53:07.104902Z",
     "shell.execute_reply": "2021-06-15T06:53:07.104337Z"
    },
    "papermill": {
     "duration": 0.033084,
     "end_time": "2021-06-15T06:53:07.105040",
     "exception": false,
     "start_time": "2021-06-15T06:53:07.071956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre-processsing step\n",
    "# Drop the columns \n",
    "from sklearn.compose import ColumnTransformer\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['id', \n",
    "                                                                        'url_legal', \n",
    "                                                                        'license',\n",
    "                                                                        'excerpt',\n",
    "                                                                        'ner_tags',\n",
    "                                                                        'pos_tags',\n",
    "                                                                        'excerpt_proc',\n",
    "                                                                        'nlp_text'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cloudy-receipt",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:07.149302Z",
     "iopub.status.busy": "2021-06-15T06:53:07.148439Z",
     "iopub.status.idle": "2021-06-15T06:53:07.152301Z",
     "shell.execute_reply": "2021-06-15T06:53:07.151735Z"
    },
    "papermill": {
     "duration": 0.028141,
     "end_time": "2021-06-15T06:53:07.152458",
     "exception": false,
     "start_time": "2021-06-15T06:53:07.124317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([#('get_new_columns', FeatureGenerator()),\n",
    "                 ('idf_vect', IDFVectorizer()),\n",
    "                 ('pre_processing',pre_process),\n",
    "                 ('random_forest', RandomForestRegressor(bootstrap = True, max_depth = 50, \n",
    "                                                         max_features = 'auto', min_samples_leaf = 4, \n",
    "                                                         min_samples_split = 10, n_estimators = 800))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "industrial-crest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:07.195837Z",
     "iopub.status.busy": "2021-06-15T06:53:07.195187Z",
     "iopub.status.idle": "2021-06-15T06:53:07.197333Z",
     "shell.execute_reply": "2021-06-15T06:53:07.197848Z"
    },
    "papermill": {
     "duration": 0.026642,
     "end_time": "2021-06-15T06:53:07.198022",
     "exception": false,
     "start_time": "2021-06-15T06:53:07.171380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preproc_pipe = Pipeline([ ('get_new_columns', FeatureGenerator())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identified-athens",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T06:53:07.241093Z",
     "iopub.status.busy": "2021-06-15T06:53:07.240476Z",
     "iopub.status.idle": "2021-06-15T07:02:36.568372Z",
     "shell.execute_reply": "2021-06-15T07:02:36.567187Z"
    },
    "papermill": {
     "duration": 569.35094,
     "end_time": "2021-06-15T07:02:36.568551",
     "exception": false,
     "start_time": "2021-06-15T06:53:07.217611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_proc = preproc_pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opponent-elder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:02:36.619957Z",
     "iopub.status.busy": "2021-06-15T07:02:36.619188Z",
     "iopub.status.idle": "2021-06-15T07:06:48.744424Z",
     "shell.execute_reply": "2021-06-15T07:06:48.744995Z"
    },
    "papermill": {
     "duration": 252.155077,
     "end_time": "2021-06-15T07:06:48.745416",
     "exception": false,
     "start_time": "2021-06-15T07:02:36.590339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8845570713699398\n",
      "0.8978722378228816\n",
      "0.8557813114197557\n",
      "0.8418926160164949\n",
      "0.8867276393091814\n",
      "0.8491989433358431\n",
      "0.8744957423837303\n",
      "0.8766229355478\n",
      "0.8790129078279463\n",
      "0.8879148011699027\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "n_repeats = 2\n",
    "group_count = 10\n",
    "cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "                           group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "\n",
    "\n",
    "# logger.info(\"Train SVR\")\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(X_proc, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predict = pipe.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, predict, squared=False)\n",
    "    print(rmse)\n",
    "#     logger.info(\"The rmse for SVR iteration {}: {:.3f}\".format(i, rmse))\n",
    "#     logger.info(\"-------------------------------\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-oregon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:48.797712Z",
     "iopub.status.busy": "2021-06-15T07:06:48.796793Z",
     "iopub.status.idle": "2021-06-15T07:06:48.812279Z",
     "shell.execute_reply": "2021-06-15T07:06:48.811719Z"
    },
    "papermill": {
     "duration": 0.043776,
     "end_time": "2021-06-15T07:06:48.812440",
     "exception": false,
     "start_time": "2021-06-15T07:06:48.768664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "inner-favorite",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:48.881648Z",
     "iopub.status.busy": "2021-06-15T07:06:48.872638Z",
     "iopub.status.idle": "2021-06-15T07:06:50.306793Z",
     "shell.execute_reply": "2021-06-15T07:06:50.306217Z"
    },
    "papermill": {
     "duration": 1.471333,
     "end_time": "2021-06-15T07:06:50.306964",
     "exception": false,
     "start_time": "2021-06-15T07:06:48.835631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = preproc_pipe.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sonic-support",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.376310Z",
     "iopub.status.busy": "2021-06-15T07:06:50.358361Z",
     "iopub.status.idle": "2021-06-15T07:06:50.450765Z",
     "shell.execute_reply": "2021-06-15T07:06:50.450019Z"
    },
    "papermill": {
     "duration": 0.120871,
     "end_time": "2021-06-15T07:06:50.450925",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.330054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "behavioral-tongue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.503915Z",
     "iopub.status.busy": "2021-06-15T07:06:50.503268Z",
     "iopub.status.idle": "2021-06-15T07:06:50.507061Z",
     "shell.execute_reply": "2021-06-15T07:06:50.506507Z"
    },
    "papermill": {
     "duration": 0.033018,
     "end_time": "2021-06-15T07:06:50.507243",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.474225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':test['id'],'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "continued-projector",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.559369Z",
     "iopub.status.busy": "2021-06-15T07:06:50.558730Z",
     "iopub.status.idle": "2021-06-15T07:06:50.567920Z",
     "shell.execute_reply": "2021-06-15T07:06:50.567366Z"
    },
    "papermill": {
     "duration": 0.037545,
     "end_time": "2021-06-15T07:06:50.568081",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.530536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "racial-ability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.620147Z",
     "iopub.status.busy": "2021-06-15T07:06:50.619453Z",
     "iopub.status.idle": "2021-06-15T07:06:50.621470Z",
     "shell.execute_reply": "2021-06-15T07:06:50.621911Z"
    },
    "papermill": {
     "duration": 0.030344,
     "end_time": "2021-06-15T07:06:50.622111",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.591767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get importance\n",
    "# importance = pipe.steps[2][1].feature_importances_\n",
    "# # summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# # plot feature importance\n",
    "# plt.bar([x for x in range(len(importance))], importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "relative-bachelor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.672462Z",
     "iopub.status.busy": "2021-06-15T07:06:50.671763Z",
     "iopub.status.idle": "2021-06-15T07:06:50.675973Z",
     "shell.execute_reply": "2021-06-15T07:06:50.676601Z"
    },
    "papermill": {
     "duration": 0.031197,
     "end_time": "2021-06-15T07:06:50.676795",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.645598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto', 'sqrt']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "# max_depth.append(None)\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [2, 5, 10]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'random_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=pipe,\n",
    "#                                    param_distributions=random_grid,\n",
    "#                                    n_iter=5,\n",
    "#                                    scoring='neg_mean_absolute_error',\n",
    "#                                    cv=3, \n",
    "#                                    verbose=1, \n",
    "#                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "olive-inside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.727033Z",
     "iopub.status.busy": "2021-06-15T07:06:50.726445Z",
     "iopub.status.idle": "2021-06-15T07:06:50.730235Z",
     "shell.execute_reply": "2021-06-15T07:06:50.730686Z"
    },
    "papermill": {
     "duration": 0.030707,
     "end_time": "2021-06-15T07:06:50.730874",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.700167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diagnostic-format",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.783342Z",
     "iopub.status.busy": "2021-06-15T07:06:50.782664Z",
     "iopub.status.idle": "2021-06-15T07:06:50.785989Z",
     "shell.execute_reply": "2021-06-15T07:06:50.786506Z"
    },
    "papermill": {
     "duration": 0.031839,
     "end_time": "2021-06-15T07:06:50.786675",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.754836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"The best hyperparameters from Random Search are:\")\n",
    "# print(random_search.best_params_)\n",
    "# print(\"\")\n",
    "# print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "# print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sealed-forge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.837680Z",
     "iopub.status.busy": "2021-06-15T07:06:50.837026Z",
     "iopub.status.idle": "2021-06-15T07:06:50.840949Z",
     "shell.execute_reply": "2021-06-15T07:06:50.841452Z"
    },
    "papermill": {
     "duration": 0.031509,
     "end_time": "2021-06-15T07:06:50.841620",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.810111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [800]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [30, 40, 50]\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [10, 20]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [4, 6]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True]\n",
    "\n",
    "# # Create the random grid\n",
    "# parameters = {'random_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "powerful-domestic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.894094Z",
     "iopub.status.busy": "2021-06-15T07:06:50.893453Z",
     "iopub.status.idle": "2021-06-15T07:06:50.896908Z",
     "shell.execute_reply": "2021-06-15T07:06:50.897433Z"
    },
    "papermill": {
     "duration": 0.032248,
     "end_time": "2021-06-15T07:06:50.897610",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.865362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(pipe, param_grid=parameters, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "changed-singer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:50.949441Z",
     "iopub.status.busy": "2021-06-15T07:06:50.948760Z",
     "iopub.status.idle": "2021-06-15T07:06:50.952303Z",
     "shell.execute_reply": "2021-06-15T07:06:50.952819Z"
    },
    "papermill": {
     "duration": 0.03138,
     "end_time": "2021-06-15T07:06:50.952988",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.921608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sexual-looking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:51.005519Z",
     "iopub.status.busy": "2021-06-15T07:06:51.004793Z",
     "iopub.status.idle": "2021-06-15T07:06:51.008151Z",
     "shell.execute_reply": "2021-06-15T07:06:51.008668Z"
    },
    "papermill": {
     "duration": 0.031664,
     "end_time": "2021-06-15T07:06:51.008843",
     "exception": false,
     "start_time": "2021-06-15T07:06:50.977179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cleared-chambers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T07:06:51.061509Z",
     "iopub.status.busy": "2021-06-15T07:06:51.060780Z",
     "iopub.status.idle": "2021-06-15T07:06:51.064674Z",
     "shell.execute_reply": "2021-06-15T07:06:51.065224Z"
    },
    "papermill": {
     "duration": 0.031871,
     "end_time": "2021-06-15T07:06:51.065392",
     "exception": false,
     "start_time": "2021-06-15T07:06:51.033521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto', 'sqrt']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "# max_depth.append(None)\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [2, 5, 10]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'rrandom_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=pipe,\n",
    "#                                    param_distributions=random_grid,\n",
    "#                                    n_iter=5,\n",
    "#                                    scoring='neg_root_mean_squared_error',\n",
    "#                                    cv=3, \n",
    "#                                    verbose=1, \n",
    "#                                    random_state=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 868.176345,
   "end_time": "2021-06-15T07:06:53.766141",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-15T06:52:25.589796",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
