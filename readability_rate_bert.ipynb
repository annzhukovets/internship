{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "russian-condition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:37.663692Z",
     "iopub.status.busy": "2021-06-16T14:46:37.662187Z",
     "iopub.status.idle": "2021-06-16T14:46:37.670415Z",
     "shell.execute_reply": "2021-06-16T14:46:37.670823Z",
     "shell.execute_reply.started": "2021-06-16T14:32:35.476905Z"
    },
    "papermill": {
     "duration": 0.038214,
     "end_time": "2021-06-16T14:46:37.671038",
     "exception": false,
     "start_time": "2021-06-16T14:46:37.632824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afraid-might",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:37.727310Z",
     "iopub.status.busy": "2021-06-16T14:46:37.726601Z",
     "iopub.status.idle": "2021-06-16T14:46:49.938731Z",
     "shell.execute_reply": "2021-06-16T14:46:49.938208Z",
     "shell.execute_reply.started": "2021-06-16T14:32:35.485230Z"
    },
    "papermill": {
     "duration": 12.244924,
     "end_time": "2021-06-16T14:46:49.938873",
     "exception": false,
     "start_time": "2021-06-16T14:46:37.693949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "%matplotlib inline\n",
    "import os\n",
    "# os.listdir(\"../input/\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.system('pip install pytorch_pretrained_bert --no-index --find-links=\"../input/pytorch-pretrained-bert/pytorch_pretrained_bert\" ')\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forced-flesh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:49.998873Z",
     "iopub.status.busy": "2021-06-16T14:46:49.998047Z",
     "iopub.status.idle": "2021-06-16T14:46:50.000217Z",
     "shell.execute_reply": "2021-06-16T14:46:50.000620Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.161248Z"
    },
    "papermill": {
     "duration": 0.032898,
     "end_time": "2021-06-16T14:46:50.000739",
     "exception": false,
     "start_time": "2021-06-16T14:46:49.967841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "\n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optional-scene",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:50.051755Z",
     "iopub.status.busy": "2021-06-16T14:46:50.051057Z",
     "iopub.status.idle": "2021-06-16T14:46:50.053786Z",
     "shell.execute_reply": "2021-06-16T14:46:50.053399Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.170063Z"
    },
    "papermill": {
     "duration": 0.029715,
     "end_time": "2021-06-16T14:46:50.053972",
     "exception": false,
     "start_time": "2021-06-16T14:46:50.024257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "labeled-corporation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:50.109200Z",
     "iopub.status.busy": "2021-06-16T14:46:50.108407Z",
     "iopub.status.idle": "2021-06-16T14:46:50.110592Z",
     "shell.execute_reply": "2021-06-16T14:46:50.111006Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.181811Z"
    },
    "papermill": {
     "duration": 0.032787,
     "end_time": "2021-06-16T14:46:50.111123",
     "exception": false,
     "start_time": "2021-06-16T14:46:50.078336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertForSequenceRegression(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForSequenceRegression, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dense = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids,  token_type_ids=None, attention_mask=None, targets=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, return_dict=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        outputs = self.dense(pooled_output)\n",
    "        return outputs.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comic-satisfaction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:50.162393Z",
     "iopub.status.busy": "2021-06-16T14:46:50.161624Z",
     "iopub.status.idle": "2021-06-16T14:46:50.164371Z",
     "shell.execute_reply": "2021-06-16T14:46:50.163932Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.192683Z"
    },
    "papermill": {
     "duration": 0.029784,
     "end_time": "2021-06-16T14:46:50.164493",
     "exception": false,
     "start_time": "2021-06-16T14:46:50.134709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RMSELoss(outputs, targets):\n",
    "    return torch.sqrt(nn.MSELoss()(outputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "female-texture",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:50.218860Z",
     "iopub.status.busy": "2021-06-16T14:46:50.218285Z",
     "iopub.status.idle": "2021-06-16T14:46:50.908149Z",
     "shell.execute_reply": "2021-06-16T14:46:50.907512Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.201389Z"
    },
    "papermill": {
     "duration": 0.719885,
     "end_time": "2021-06-16T14:46:50.908298",
     "exception": false,
     "start_time": "2021-06-16T14:46:50.188413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "class regressor_stratified_cv:\n",
    "    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n",
    "                 random_state = 0, strategy = 'quantile'):\n",
    "        self.group_count = group_count\n",
    "        self.strategy = strategy\n",
    "        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n",
    "                             random_state = random_state)\n",
    "        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n",
    "        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n",
    "                                            strategy = self.strategy)  \n",
    "            \n",
    "    def split(self, X, y, groups = None):\n",
    "        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n",
    "        return self.cv.split(X, kgroups, groups)\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups = None):\n",
    "        return self.cv.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specified-burner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:50.961681Z",
     "iopub.status.busy": "2021-06-16T14:46:50.960861Z",
     "iopub.status.idle": "2021-06-16T14:46:50.977501Z",
     "shell.execute_reply": "2021-06-16T14:46:50.977983Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.215261Z"
    },
    "papermill": {
     "duration": 0.046139,
     "end_time": "2021-06-16T14:46:50.978199",
     "exception": false,
     "start_time": "2021-06-16T14:46:50.932060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(excerpt):\n",
    "    \n",
    "    # lower casing\n",
    "    excerpt = excerpt.lower()\n",
    "\n",
    "    # removal of punctuation\n",
    "    excerpt = excerpt.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        \n",
    "    # removal of stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    \", \".join(stopwords.words('english'))\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    excerpt = \" \".join([word for word in str(excerpt).split() if word not in STOPWORDS])\n",
    "        \n",
    "    # lemmatization \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    excerpt = \" \".join([lemmatizer.lemmatize(word) for word in excerpt.split()])\n",
    "        \n",
    "                \n",
    "    return excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "productive-cargo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:51.068386Z",
     "iopub.status.busy": "2021-06-16T14:46:51.067197Z",
     "iopub.status.idle": "2021-06-16T14:46:51.202581Z",
     "shell.execute_reply": "2021-06-16T14:46:51.203537Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.227423Z"
    },
    "papermill": {
     "duration": 0.183998,
     "end_time": "2021-06-16T14:46:51.203733",
     "exception": false,
     "start_time": "2021-06-16T14:46:51.019735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2834 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2829  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2830  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2832  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2833  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2829  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2830  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2831  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2832  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2833  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2829  1.711390        0.646900  \n",
       "2830  0.189476        0.535648  \n",
       "2831  0.255209        0.483866  \n",
       "2832 -0.215279        0.514128  \n",
       "2833  0.300779        0.512379  \n",
       "\n",
       "[2834 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dying-lexington",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:51.289518Z",
     "iopub.status.busy": "2021-06-16T14:46:51.288236Z",
     "iopub.status.idle": "2021-06-16T14:46:55.604891Z",
     "shell.execute_reply": "2021-06-16T14:46:55.604032Z",
     "shell.execute_reply.started": "2021-06-16T14:32:40.274419Z"
    },
    "papermill": {
     "duration": 4.361501,
     "end_time": "2021-06-16T14:46:55.605055",
     "exception": false,
     "start_time": "2021-06-16T14:46:51.243554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"excerpt\"].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "voluntary-auction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:55.657551Z",
     "iopub.status.busy": "2021-06-16T14:46:55.656724Z",
     "iopub.status.idle": "2021-06-16T14:46:55.659374Z",
     "shell.execute_reply": "2021-06-16T14:46:55.658789Z",
     "shell.execute_reply.started": "2021-06-16T14:32:42.538395Z"
    },
    "papermill": {
     "duration": 0.030184,
     "end_time": "2021-06-16T14:46:55.659493",
     "exception": false,
     "start_time": "2021-06-16T14:46:55.629309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "excerpts = df.text.values\n",
    "targets = df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "novel-postage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:55.710032Z",
     "iopub.status.busy": "2021-06-16T14:46:55.709269Z",
     "iopub.status.idle": "2021-06-16T14:46:55.711382Z",
     "shell.execute_reply": "2021-06-16T14:46:55.711857Z",
     "shell.execute_reply.started": "2021-06-16T14:32:42.545494Z"
    },
    "papermill": {
     "duration": 0.029174,
     "end_time": "2021-06-16T14:46:55.711980",
     "exception": false,
     "start_time": "2021-06-16T14:46:55.682806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BERT_FP = '../input/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aggressive-london",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:55.762850Z",
     "iopub.status.busy": "2021-06-16T14:46:55.762366Z",
     "iopub.status.idle": "2021-06-16T14:46:55.810313Z",
     "shell.execute_reply": "2021-06-16T14:46:55.809846Z",
     "shell.execute_reply.started": "2021-06-16T14:32:42.554196Z"
    },
    "papermill": {
     "duration": 0.074546,
     "end_time": "2021-06-16T14:46:55.810435",
     "exception": false,
     "start_time": "2021-06-16T14:46:55.735889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_FP, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "weird-event",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:46:55.886087Z",
     "iopub.status.busy": "2021-06-16T14:46:55.870656Z",
     "iopub.status.idle": "2021-06-16T14:47:05.820844Z",
     "shell.execute_reply": "2021-06-16T14:47:05.819899Z",
     "shell.execute_reply.started": "2021-06-16T14:32:42.601218Z"
    },
    "papermill": {
     "duration": 9.986248,
     "end_time": "2021-06-16T14:47:05.821001",
     "exception": false,
     "start_time": "2021-06-16T14:46:55.834753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert sentences into tokens\n",
    "input_ids = [tokenizer.encode(excerpt, add_special_tokens = True, max_length = MAX_LENGTH,\n",
    "                              padding='max_length') for excerpt in excerpts]\n",
    "\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = []\n",
    "# create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "attention_masks = np.array(attention_masks)\n",
    "# create token type ids\n",
    "token_type_ids = [[0 for i in seq] for seq in input_ids]\n",
    "token_type_ids = np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pressed-steps",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:05.873806Z",
     "iopub.status.busy": "2021-06-16T14:47:05.872909Z",
     "iopub.status.idle": "2021-06-16T14:47:05.875199Z",
     "shell.execute_reply": "2021-06-16T14:47:05.875568Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.259210Z"
    },
    "papermill": {
     "duration": 0.030329,
     "end_time": "2021-06-16T14:47:05.875694",
     "exception": false,
     "start_time": "2021-06-16T14:47:05.845365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "# n_repeats = 2\n",
    "# group_count = 10\n",
    "# cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "#                            group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "\n",
    "# for train_index, test_index in cv.split(input_ids, targets):\n",
    "#     train_inputs, test_inputs = input_ids[train_index], input_ids[test_index]\n",
    "#     train_targets, test_targets = targets[train_index], targets[test_index]\n",
    "#     train_masks, test_masks = attention_masks[train_index], attention_masks[test_index]\n",
    "#     train_type_ids, test_type_ids = token_type_ids[train_index], token_type_ids[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surrounded-whale",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:05.926331Z",
     "iopub.status.busy": "2021-06-16T14:47:05.925786Z",
     "iopub.status.idle": "2021-06-16T14:47:05.929731Z",
     "shell.execute_reply": "2021-06-16T14:47:05.929270Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.266007Z"
    },
    "papermill": {
     "duration": 0.030261,
     "end_time": "2021-06-16T14:47:05.929834",
     "exception": false,
     "start_time": "2021-06-16T14:47:05.899573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n",
    "# test_inputs = torch.tensor(test_inputs, dtype=torch.long)\n",
    "# train_targets = torch.tensor(train_targets, dtype=torch.float)\n",
    "# test_targets = torch.tensor(test_targets, dtype=torch.float)\n",
    "# train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "# test_masks = torch.tensor(test_masks, dtype=torch.long)\n",
    "# train_type_ids = torch.tensor(train_type_ids, dtype=torch.long)\n",
    "# test_type_ids = torch.tensor(test_type_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "standing-exercise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:05.982234Z",
     "iopub.status.busy": "2021-06-16T14:47:05.981503Z",
     "iopub.status.idle": "2021-06-16T14:47:05.984123Z",
     "shell.execute_reply": "2021-06-16T14:47:05.983662Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.275203Z"
    },
    "papermill": {
     "duration": 0.03016,
     "end_time": "2021-06-16T14:47:05.984228",
     "exception": false,
     "start_time": "2021-06-16T14:47:05.954068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data = TensorDataset(train_inputs, train_masks, train_type_ids, train_targets)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "# test_data = TensorDataset(test_inputs, test_masks, test_type_ids, test_targets)\n",
    "# test_sampler = RandomSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charming-count",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:06.034674Z",
     "iopub.status.busy": "2021-06-16T14:47:06.034195Z",
     "iopub.status.idle": "2021-06-16T14:47:06.038000Z",
     "shell.execute_reply": "2021-06-16T14:47:06.037547Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.283998Z"
    },
    "papermill": {
     "duration": 0.030072,
     "end_time": "2021-06-16T14:47:06.038102",
     "exception": false,
     "start_time": "2021-06-16T14:47:06.008030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = BertForSequenceRegression.from_pretrained(\n",
    "#     BERT_FP, \n",
    "# )\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reflected-sussex",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:06.089937Z",
     "iopub.status.busy": "2021-06-16T14:47:06.089123Z",
     "iopub.status.idle": "2021-06-16T14:47:06.092155Z",
     "shell.execute_reply": "2021-06-16T14:47:06.091721Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.296559Z"
    },
    "papermill": {
     "duration": 0.030315,
     "end_time": "2021-06-16T14:47:06.092255",
     "exception": false,
     "start_time": "2021-06-16T14:47:06.061940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get all of the model's parameters as a list of tuples.\n",
    "# params = list(model.named_parameters())\n",
    "\n",
    "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "# print('==== Embedding Layer ====\\n')\n",
    "\n",
    "# for p in params[0:5]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "# for p in params[5:21]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "# for p in params[-4:]:\n",
    "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dominant-douglas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:06.143630Z",
     "iopub.status.busy": "2021-06-16T14:47:06.142839Z",
     "iopub.status.idle": "2021-06-16T14:47:06.145720Z",
     "shell.execute_reply": "2021-06-16T14:47:06.145311Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.306285Z"
    },
    "papermill": {
     "duration": 0.029757,
     "end_time": "2021-06-16T14:47:06.145815",
     "exception": false,
     "start_time": "2021-06-16T14:47:06.116058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(),\n",
    "#                   lr = 5e-4,\n",
    "#                   eps = 1e-6 \n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "improving-jumping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:06.197244Z",
     "iopub.status.busy": "2021-06-16T14:47:06.196531Z",
     "iopub.status.idle": "2021-06-16T14:47:06.199276Z",
     "shell.execute_reply": "2021-06-16T14:47:06.198852Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.315109Z"
    },
    "papermill": {
     "duration": 0.029727,
     "end_time": "2021-06-16T14:47:06.199399",
     "exception": false,
     "start_time": "2021-06-16T14:47:06.169672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps = 0,\n",
    "#                                             num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "diverse-student",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:06.251465Z",
     "iopub.status.busy": "2021-06-16T14:47:06.250234Z",
     "iopub.status.idle": "2021-06-16T14:47:06.252595Z",
     "shell.execute_reply": "2021-06-16T14:47:06.253050Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.324354Z"
    },
    "papermill": {
     "duration": 0.029742,
     "end_time": "2021-06-16T14:47:06.253163",
     "exception": false,
     "start_time": "2021-06-16T14:47:06.223421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train only last layers\n",
    "# set_trainable(model, True)\n",
    "# set_trainable(model.bert.embeddings, False)\n",
    "# set_trainable(model.bert.encoder, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "developed-simple",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:47:06.319050Z",
     "iopub.status.busy": "2021-06-16T14:47:06.318475Z",
     "iopub.status.idle": "2021-06-16T14:59:16.243628Z",
     "shell.execute_reply": "2021-06-16T14:59:16.244353Z",
     "shell.execute_reply.started": "2021-06-16T14:32:52.333677Z"
    },
    "papermill": {
     "duration": 729.967776,
     "end_time": "2021-06-16T14:59:16.244565",
     "exception": false,
     "start_time": "2021-06-16T14:47:06.276789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Iter 0  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.7972164011337388\n",
      "Eval loss:  0.6682190299034119\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6967094733681477\n",
      "Eval loss:  0.656562089920044\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6625640123662814\n",
      "Eval loss:  0.5700247287750244\n",
      "======== Iter 1  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.7894523588704391\n",
      "Eval loss:  0.7137970924377441\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6785855305866456\n",
      "Eval loss:  0.5812800526618958\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6748593159964387\n",
      "Eval loss:  0.8483160138130188\n",
      "======== Iter 2  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.7707036523751809\n",
      "Eval loss:  0.8255541324615479\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6896791491709965\n",
      "Eval loss:  0.6196920275688171\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6628590687899523\n",
      "Eval loss:  0.693461000919342\n",
      "======== Iter 3  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.8215976588323083\n",
      "Eval loss:  0.7648919820785522\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6786160494240236\n",
      "Eval loss:  0.7503983974456787\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6651355775309281\n",
      "Eval loss:  0.7304264903068542\n",
      "======== Iter 4  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.77788383188382\n",
      "Eval loss:  0.9693148136138916\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6895771647842837\n",
      "Eval loss:  0.7932246327400208\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6565069216237941\n",
      "Eval loss:  0.5975861549377441\n",
      "======== Iter 5  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.7841816275892123\n",
      "Eval loss:  0.6953079104423523\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6772546029426683\n",
      "Eval loss:  0.526301383972168\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6525075087245081\n",
      "Eval loss:  0.8654531836509705\n",
      "======== Iter 6  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.7955405846448012\n",
      "Eval loss:  0.9006986021995544\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6857070624828339\n",
      "Eval loss:  0.8065899610519409\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6549597865259144\n",
      "Eval loss:  0.9306551814079285\n",
      "======== Iter 7  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.8343961826512512\n",
      "Eval loss:  0.6409093141555786\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6988520269662561\n",
      "Eval loss:  0.5136734843254089\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6647941642244097\n",
      "Eval loss:  0.6631618738174438\n",
      "======== Iter 8  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.7781297933887428\n",
      "Eval loss:  0.722335934638977\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6720603305689046\n",
      "Eval loss:  0.6258094310760498\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6681889337553105\n",
      "Eval loss:  0.5139335989952087\n",
      "======== Iter 9  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Train loss:  0.8155749431798156\n",
      "Eval loss:  0.837369441986084\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Train loss:  0.6980814635753632\n",
      "Eval loss:  0.5661002397537231\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Train loss:  0.6710099183337789\n",
      "Eval loss:  0.8125994205474854\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "n_repeats = 2\n",
    "group_count = 10\n",
    "cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "                           group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "X = input_ids\n",
    "y = targets\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    print('======== Iter {:}  ========'.format(i))\n",
    "    train_inputs, test_inputs = X[train_index], X[test_index]\n",
    "    train_targets, test_targets = y[train_index], y[test_index]\n",
    "    train_masks, test_masks = attention_masks[train_index], attention_masks[test_index]\n",
    "    train_type_ids, test_type_ids = token_type_ids[train_index], token_type_ids[test_index]\n",
    "    \n",
    "    train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n",
    "    test_inputs = torch.tensor(test_inputs, dtype=torch.long)\n",
    "    train_targets = torch.tensor(train_targets, dtype=torch.float)\n",
    "    test_targets = torch.tensor(test_targets, dtype=torch.float)\n",
    "    train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "    test_masks = torch.tensor(test_masks, dtype=torch.long)\n",
    "    train_type_ids = torch.tensor(train_type_ids, dtype=torch.long)\n",
    "    test_type_ids = torch.tensor(test_type_ids, dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_type_ids, train_targets)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_type_ids, test_targets)\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    model = BertForSequenceRegression.from_pretrained(BERT_FP)\n",
    "    model.to(device)\n",
    "    set_trainable(model, True)\n",
    "    set_trainable(model.bert.embeddings, False)\n",
    "    set_trainable(model.bert.encoder, False)\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-4,\n",
    "                  eps = 1e-6 \n",
    "                )\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        # training\n",
    "        model.train()\n",
    "        tr_loss = []\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, type_ids, target = batch\n",
    "            output = model(input_ids, input_mask, type_ids, target)\n",
    "            loss = RMSELoss(output, target)\n",
    "            tr_loss.append(loss.cpu().detach().numpy().tolist())\n",
    "            loss.backward()  \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  \n",
    "            scheduler.step()\n",
    "            \n",
    "        train_losses = np.mean(tr_loss)  \n",
    "        print(\"Train loss: \", train_losses)\n",
    "        all_targets, all_preds = [], []\n",
    "        model.eval()   \n",
    "        eval_loss = []\n",
    "        # evaluation\n",
    "        # disable gradients \n",
    "        with torch.no_grad(): \n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, type_ids, target = batch\n",
    "                output = model(input_ids, input_mask, type_ids, target)\n",
    "                loss = RMSELoss(output, target)\n",
    "            eval_loss.append(loss.cpu().detach().numpy().tolist())\n",
    "            \n",
    "        epoch_eval_loss = np.mean(eval_loss)\n",
    "        print(\"Eval loss: \", epoch_eval_loss)\n",
    "    i += 1    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "respiratory-translation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.345349Z",
     "iopub.status.busy": "2021-06-16T14:59:16.344823Z",
     "iopub.status.idle": "2021-06-16T14:59:16.352737Z",
     "shell.execute_reply": "2021-06-16T14:59:16.352335Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.289209Z"
    },
    "papermill": {
     "duration": 0.060778,
     "end_time": "2021-06-16T14:59:16.352845",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.292067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "differential-sterling",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.450669Z",
     "iopub.status.busy": "2021-06-16T14:59:16.449856Z",
     "iopub.status.idle": "2021-06-16T14:59:16.459083Z",
     "shell.execute_reply": "2021-06-16T14:59:16.458664Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.307003Z"
    },
    "papermill": {
     "duration": 0.060223,
     "end_time": "2021-06-16T14:59:16.459192",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.398969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"excerpt\"].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "satisfactory-chase",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.556527Z",
     "iopub.status.busy": "2021-06-16T14:59:16.555967Z",
     "iopub.status.idle": "2021-06-16T14:59:16.559739Z",
     "shell.execute_reply": "2021-06-16T14:59:16.559339Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.322780Z"
    },
    "papermill": {
     "duration": 0.054503,
     "end_time": "2021-06-16T14:59:16.559842",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.505339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "excerpts = test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "legal-secondary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.683950Z",
     "iopub.status.busy": "2021-06-16T14:59:16.682743Z",
     "iopub.status.idle": "2021-06-16T14:59:16.684967Z",
     "shell.execute_reply": "2021-06-16T14:59:16.685420Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.331438Z"
    },
    "papermill": {
     "duration": 0.079675,
     "end_time": "2021-06-16T14:59:16.685539",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.605864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert sentences into tokens\n",
    "input_ids = [tokenizer.encode(excerpt, add_special_tokens = True, max_length = MAX_LENGTH,\n",
    "                              padding='max_length') for excerpt in excerpts]\n",
    "\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = []\n",
    "# create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "attention_masks = np.array(attention_masks)\n",
    "# create token type ids\n",
    "token_type_ids = [[0 for i in seq] for seq in input_ids]\n",
    "token_type_ids = np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "impossible-creature",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.782482Z",
     "iopub.status.busy": "2021-06-16T14:59:16.781811Z",
     "iopub.status.idle": "2021-06-16T14:59:16.784653Z",
     "shell.execute_reply": "2021-06-16T14:59:16.784228Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.365621Z"
    },
    "papermill": {
     "duration": 0.052994,
     "end_time": "2021-06-16T14:59:16.784750",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.731756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "limiting-delicious",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.881579Z",
     "iopub.status.busy": "2021-06-16T14:59:16.881086Z",
     "iopub.status.idle": "2021-06-16T14:59:16.884375Z",
     "shell.execute_reply": "2021-06-16T14:59:16.884776Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.373367Z"
    },
    "papermill": {
     "duration": 0.054002,
     "end_time": "2021-06-16T14:59:16.884903",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.830901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_data = TensorDataset(input_ids, attention_masks, token_type_ids)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "czech-syria",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:16.984676Z",
     "iopub.status.busy": "2021-06-16T14:59:16.983812Z",
     "iopub.status.idle": "2021-06-16T14:59:17.046515Z",
     "shell.execute_reply": "2021-06-16T14:59:17.046030Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.383307Z"
    },
    "papermill": {
     "duration": 0.114675,
     "end_time": "2021-06-16T14:59:17.046630",
     "exception": false,
     "start_time": "2021-06-16T14:59:16.931955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()   \n",
    "predictions , true_labels = [], []\n",
    "# evaluation\n",
    "for batch in prediction_dataloader:\n",
    "    # disable gradients \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, type_ids = batch\n",
    "    with torch.no_grad():    \n",
    "        output = model(input_ids, input_mask, type_ids) \n",
    "    output = output.cpu().detach().numpy().tolist()\n",
    "    predictions += output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unlikely-album",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:17.145012Z",
     "iopub.status.busy": "2021-06-16T14:59:17.144344Z",
     "iopub.status.idle": "2021-06-16T14:59:17.147180Z",
     "shell.execute_reply": "2021-06-16T14:59:17.146653Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.452596Z"
    },
    "papermill": {
     "duration": 0.053246,
     "end_time": "2021-06-16T14:59:17.147295",
     "exception": false,
     "start_time": "2021-06-16T14:59:17.094049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':test['id'],'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hazardous-queen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:17.243078Z",
     "iopub.status.busy": "2021-06-16T14:59:17.242380Z",
     "iopub.status.idle": "2021-06-16T14:59:17.361625Z",
     "shell.execute_reply": "2021-06-16T14:59:17.361201Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.459670Z"
    },
    "papermill": {
     "duration": 0.168092,
     "end_time": "2021-06-16T14:59:17.361736",
     "exception": false,
     "start_time": "2021-06-16T14:59:17.193644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "integral-vietnamese",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:17.460429Z",
     "iopub.status.busy": "2021-06-16T14:59:17.459768Z",
     "iopub.status.idle": "2021-06-16T14:59:17.463265Z",
     "shell.execute_reply": "2021-06-16T14:59:17.462830Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.681538Z"
    },
    "papermill": {
     "duration": 0.055007,
     "end_time": "2021-06-16T14:59:17.463366",
     "exception": false,
     "start_time": "2021-06-16T14:59:17.408359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "transparent-google",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T14:59:17.559513Z",
     "iopub.status.busy": "2021-06-16T14:59:17.558971Z",
     "iopub.status.idle": "2021-06-16T14:59:17.562679Z",
     "shell.execute_reply": "2021-06-16T14:59:17.562228Z",
     "shell.execute_reply.started": "2021-06-16T14:44:52.688630Z"
    },
    "papermill": {
     "duration": 0.053536,
     "end_time": "2021-06-16T14:59:17.562779",
     "exception": false,
     "start_time": "2021-06-16T14:59:17.509243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "# token_type_ids = []\n",
    "\n",
    "# for excerpt in excerpts:\n",
    "#     encoded_dict = tokenizer.encode_plus(\n",
    "#                         excerpt,                      \n",
    "#                         add_special_tokens = True, \n",
    "#                         max_length = MAX_LENGTH,           \n",
    "#                         padding = \"max_length\"\n",
    "#                    )\n",
    "    \n",
    "       \n",
    "#     input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "#     attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "#     token_type_ids.append(encoded_dict[\"token_type_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 769.887957,
   "end_time": "2021-06-16T14:59:20.541361",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-16T14:46:30.653404",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
