{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import string\n%matplotlib inline\nimport os\n# os.listdir(\"../input/\")\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nos.system('pip install pytorch_pretrained_bert --no-index --find-links=\"../input/pytorch-pretrained-bert/pytorch_pretrained_bert\" ')\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nimport random\nfrom transformers import get_linear_schedule_with_warmup\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nBATCH_SIZE = 32\nMAX_LENGTH = 256\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-18T11:55:03.098900Z","iopub.execute_input":"2021-06-18T11:55:03.099337Z","iopub.status.idle":"2021-06-18T11:55:07.861940Z","shell.execute_reply.started":"2021-06-18T11:55:03.099298Z","shell.execute_reply":"2021-06-18T11:55:07.860880Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"def children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n\ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.863874Z","iopub.execute_input":"2021-06-18T11:55:07.864215Z","iopub.status.idle":"2021-06-18T11:55:07.871406Z","shell.execute_reply.started":"2021-06-18T11:55:07.864176Z","shell.execute_reply":"2021-06-18T11:55:07.870281Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"class BertForSequenceRegression(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertForSequenceRegression, self).__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.linear1 = nn.Linear(config.hidden_size, 256)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(256, 1)\n\n    def forward(self, ids,  token_type_ids=None, attention_mask=None, targets=None):\n        _, pooled_output = self.bert(ids, token_type_ids, attention_mask, return_dict=False)\n        pooled_output = self.dropout(pooled_output)\n        pooled_output = self.linear1(pooled_output)\n        pooled_output = self.relu(pooled_output)\n        outputs = self.linear2(pooled_output)\n        return outputs.view(-1)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.873379Z","iopub.execute_input":"2021-06-18T11:55:07.873716Z","iopub.status.idle":"2021-06-18T11:55:07.883455Z","shell.execute_reply.started":"2021-06-18T11:55:07.873679Z","shell.execute_reply":"2021-06-18T11:55:07.882651Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"def RMSELoss(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs, targets))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.884756Z","iopub.execute_input":"2021-06-18T11:55:07.884996Z","iopub.status.idle":"2021-06-18T11:55:07.899035Z","shell.execute_reply.started":"2021-06-18T11:55:07.884974Z","shell.execute_reply":"2021-06-18T11:55:07.898204Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nclass regressor_stratified_cv:\n    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n                 random_state = 0, strategy = 'quantile'):\n        self.group_count = group_count\n        self.strategy = strategy\n        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n                             random_state = random_state)\n        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n                                            strategy = self.strategy)  \n            \n    def split(self, X, y, groups = None):\n        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n        return self.cv.split(X, kgroups, groups)\n    \n    def get_n_splits(self, X, y, groups = None):\n        return self.cv.get_n_splits(X, y, groups)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.900433Z","iopub.execute_input":"2021-06-18T11:55:07.900851Z","iopub.status.idle":"2021-06-18T11:55:07.909768Z","shell.execute_reply.started":"2021-06-18T11:55:07.900818Z","shell.execute_reply":"2021-06-18T11:55:07.908746Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(excerpt):\n    \n    # lower casing\n    excerpt = excerpt.lower()\n\n    # removal of punctuation\n    excerpt = excerpt.translate(str.maketrans('', '', string.punctuation))\n\n        \n    # removal of stopwords\n    from nltk.corpus import stopwords\n    \", \".join(stopwords.words('english'))\n    STOPWORDS = set(stopwords.words('english'))\n    excerpt = \" \".join([word for word in str(excerpt).split() if word not in STOPWORDS])\n        \n    # lemmatization \n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    excerpt = \" \".join([lemmatizer.lemmatize(word) for word in excerpt.split()])\n        \n                \n    return excerpt","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.911053Z","iopub.execute_input":"2021-06-18T11:55:07.911501Z","iopub.status.idle":"2021-06-18T11:55:07.920445Z","shell.execute_reply.started":"2021-06-18T11:55:07.911464Z","shell.execute_reply":"2021-06-18T11:55:07.919517Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.921462Z","iopub.execute_input":"2021-06-18T11:55:07.921720Z","iopub.status.idle":"2021-06-18T11:55:07.974851Z","shell.execute_reply.started":"2021-06-18T11:55:07.921697Z","shell.execute_reply":"2021-06-18T11:55:07.974137Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"             id                                          url_legal  \\\n0     c12129c31                                                NaN   \n1     85aa80a4c                                                NaN   \n2     b69ac6792                                                NaN   \n3     dd1000b26                                                NaN   \n4     37c1b32fb                                                NaN   \n...         ...                                                ...   \n2829  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n2830  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n2831  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n2832  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n2833  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n\n           license                                            excerpt  \\\n0              NaN  When the young people returned to the ballroom...   \n1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n2              NaN  As Roger had predicted, the snow departed as q...   \n3              NaN  And outside before the palace a great garden w...   \n4              NaN  Once upon a time there were Three Bears who li...   \n...            ...                                                ...   \n2829  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n2830  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n2831  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n2832  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n2833  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n\n        target  standard_error  \n0    -0.340259        0.464009  \n1    -0.315372        0.480805  \n2    -0.580118        0.476676  \n3    -1.054013        0.450007  \n4     0.247197        0.510845  \n...        ...             ...  \n2829  1.711390        0.646900  \n2830  0.189476        0.535648  \n2831  0.255209        0.483866  \n2832 -0.215279        0.514128  \n2833  0.300779        0.512379  \n\n[2834 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url_legal</th>\n      <th>license</th>\n      <th>excerpt</th>\n      <th>target</th>\n      <th>standard_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c12129c31</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>When the young people returned to the ballroom...</td>\n      <td>-0.340259</td>\n      <td>0.464009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85aa80a4c</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n      <td>-0.315372</td>\n      <td>0.480805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b69ac6792</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>As Roger had predicted, the snow departed as q...</td>\n      <td>-0.580118</td>\n      <td>0.476676</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dd1000b26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>And outside before the palace a great garden w...</td>\n      <td>-1.054013</td>\n      <td>0.450007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37c1b32fb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Once upon a time there were Three Bears who li...</td>\n      <td>0.247197</td>\n      <td>0.510845</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2829</th>\n      <td>25ca8f498</td>\n      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n      <td>CC BY-SA 3.0</td>\n      <td>When you think of dinosaurs and where they liv...</td>\n      <td>1.711390</td>\n      <td>0.646900</td>\n    </tr>\n    <tr>\n      <th>2830</th>\n      <td>2c26db523</td>\n      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n      <td>CC BY-SA 3.0</td>\n      <td>So what is a solid? Solids are usually hard be...</td>\n      <td>0.189476</td>\n      <td>0.535648</td>\n    </tr>\n    <tr>\n      <th>2831</th>\n      <td>cd19e2350</td>\n      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n      <td>CC BY-SA 3.0</td>\n      <td>The second state of matter we will discuss is ...</td>\n      <td>0.255209</td>\n      <td>0.483866</td>\n    </tr>\n    <tr>\n      <th>2832</th>\n      <td>15e2e9e7a</td>\n      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n      <td>CC BY-SA 3.0</td>\n      <td>Solids are shapes that you can actually touch....</td>\n      <td>-0.215279</td>\n      <td>0.514128</td>\n    </tr>\n    <tr>\n      <th>2833</th>\n      <td>5b990ba77</td>\n      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n      <td>CC BY-SA 3.0</td>\n      <td>Animals are made of many cells. They eat thing...</td>\n      <td>0.300779</td>\n      <td>0.512379</td>\n    </tr>\n  </tbody>\n</table>\n<p>2834 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df[\"text\"] = df[\"excerpt\"].apply(lambda x: text_preprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:07.975916Z","iopub.execute_input":"2021-06-18T11:55:07.976232Z","iopub.status.idle":"2021-06-18T11:55:10.581996Z","shell.execute_reply.started":"2021-06-18T11:55:07.976201Z","shell.execute_reply":"2021-06-18T11:55:10.581057Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"excerpts = df.text.values\ntargets = df.target.values","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:10.585592Z","iopub.execute_input":"2021-06-18T11:55:10.585849Z","iopub.status.idle":"2021-06-18T11:55:10.590476Z","shell.execute_reply.started":"2021-06-18T11:55:10.585823Z","shell.execute_reply":"2021-06-18T11:55:10.589336Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"BERT_FP = '../input/bert-base-uncased'","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:10.592721Z","iopub.execute_input":"2021-06-18T11:55:10.593062Z","iopub.status.idle":"2021-06-18T11:55:10.600937Z","shell.execute_reply.started":"2021-06-18T11:55:10.593025Z","shell.execute_reply":"2021-06-18T11:55:10.600030Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# Load the BERT tokenizer.\ntokenizer = BertTokenizer.from_pretrained(BERT_FP, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:10.602179Z","iopub.execute_input":"2021-06-18T11:55:10.602675Z","iopub.status.idle":"2021-06-18T11:55:10.647510Z","shell.execute_reply.started":"2021-06-18T11:55:10.602641Z","shell.execute_reply":"2021-06-18T11:55:10.646790Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# convert sentences into tokens\ninput_ids = [tokenizer.encode(excerpt, add_special_tokens = True, max_length = MAX_LENGTH,\n                              padding='max_length') for excerpt in excerpts]\n\ninput_ids = np.array(input_ids)\nattention_masks = []\n# create a mask of 1 for all input tokens and 0 for all padding tokens\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]\nattention_masks = np.array(attention_masks)\n# create token type ids\ntoken_type_ids = [[0 for i in seq] for seq in input_ids]\ntoken_type_ids = np.array(token_type_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:10.648684Z","iopub.execute_input":"2021-06-18T11:55:10.649002Z","iopub.status.idle":"2021-06-18T11:55:20.604730Z","shell.execute_reply.started":"2021-06-18T11:55:10.648970Z","shell.execute_reply":"2021-06-18T11:55:20.603856Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"n_splits = 5\nn_repeats = 1\ngroup_count = 10\n# n_epochs_stop = 3\nepochs = 5\n# epochs_no_improve = 0\ncv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n                           group_count = group_count, random_state = 0, strategy = 'quantile')\n\ni = 0\neval_losses = []\nfor train_index, test_index in cv.split(input_ids, targets):\n    print('======== Iter {:}  ========'.format(i))\n    train_inputs, test_inputs = input_ids[train_index], input_ids[test_index]\n    train_targets, test_targets = targets[train_index], targets[test_index]\n    train_masks, test_masks = attention_masks[train_index], attention_masks[test_index]\n    train_type_ids, test_type_ids = token_type_ids[train_index], token_type_ids[test_index]\n    \n    train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n    test_inputs = torch.tensor(test_inputs, dtype=torch.long)\n    train_targets = torch.tensor(train_targets, dtype=torch.float)\n    test_targets = torch.tensor(test_targets, dtype=torch.float)\n    train_masks = torch.tensor(train_masks, dtype=torch.long)\n    test_masks = torch.tensor(test_masks, dtype=torch.long)\n    train_type_ids = torch.tensor(train_type_ids, dtype=torch.long)\n    test_type_ids = torch.tensor(test_type_ids, dtype=torch.long)\n    \n    train_data = TensorDataset(train_inputs, train_masks, train_type_ids, train_targets)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n\n    test_data = TensorDataset(test_inputs, test_masks, test_type_ids, test_targets)\n    test_sampler = RandomSampler(test_data)\n    test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)\n    \n    model = BertForSequenceRegression.from_pretrained(BERT_FP)\n    model.to(device)\n    set_trainable(model, True)\n    set_trainable(model.bert.embeddings, False)\n    set_trainable(model.bert.encoder, False)\n    optimizer = AdamW(model.parameters(),\n                  lr = 5e-4,\n                  eps = 1e-6 \n                )\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\n    iter_eval_loss = []\n#     min_eval_loss = np.Inf\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        # training\n        model.train()\n        tr_loss = []\n        \n        for step, batch in enumerate(train_dataloader):\n            batch = tuple(t.to(device) for t in batch)\n            ids, input_mask, type_ids, target = batch\n            output = model(ids, input_mask, type_ids, target)\n            loss = RMSELoss(output, target)\n            tr_loss.append(loss.cpu().detach().numpy().tolist())\n            loss.backward()  \n            optimizer.step()\n            optimizer.zero_grad()  \n            scheduler.step()\n            \n        train_losses = np.mean(tr_loss)  \n        print(\"Train loss: \", train_losses)\n        all_targets, all_preds = [], []\n        model.eval()   \n        eval_loss = []\n        # evaluation\n        # disable gradients \n        with torch.no_grad(): \n            for batch in test_dataloader:\n                batch = tuple(t.to(device) for t in batch)\n                ids, input_mask, type_ids, target = batch\n                output = model(ids, input_mask, type_ids, target)\n                loss = RMSELoss(output, target)\n            eval_loss.append(loss.cpu().detach().numpy().tolist())\n            \n        epoch_eval_loss = np.mean(eval_loss)\n        print(\"Eval loss: \", epoch_eval_loss)\n        iter_eval_loss.append(epoch_eval_loss) \n#         if epoch_eval_loss < min_eval_loss:\n#             epochs_no_improve = 0\n#             min_eval_loss = epoch_eval_loss\n#         else:\n#             epochs_no_improve += 1\n#         if epoch_i > 4 and epochs_no_improve >= n_epochs_stop:\n#             print('Early stopping! Epoch {:}'.format(epoch_i) )\n#             break\n#         else:\n#             continue\n          \n    iter_eval_loss = np.mean(iter_eval_loss)\n    print(\"Iter eval loss: \", iter_eval_loss)\n    eval_losses.append(iter_eval_loss)     \n    i += 1    \n    torch.cuda.empty_cache()\nmean_eval_loss = np.mean(eval_losses)\nprint(mean_eval_loss)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:55:20.606065Z","iopub.execute_input":"2021-06-18T11:55:20.606409Z","iopub.status.idle":"2021-06-18T12:05:14.635316Z","shell.execute_reply.started":"2021-06-18T11:55:20.606372Z","shell.execute_reply":"2021-06-18T12:05:14.633937Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"======== Iter 0  ========\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTrain loss:  0.8519517334414201\nEval loss:  0.6968808770179749\n\n======== Epoch 2 / 5 ========\nTrain loss:  0.6969663878561745\nEval loss:  0.8589928150177002\n\n======== Epoch 3 / 5 ========\nTrain loss:  0.6483965054364271\nEval loss:  0.6154387593269348\n\n======== Epoch 4 / 5 ========\nTrain loss:  0.6539863654425446\nEval loss:  0.7765675783157349\n\n======== Epoch 5 / 5 ========\nTrain loss:  0.6372475619886963\nEval loss:  0.736697256565094\nIter eval loss:  0.7369154572486878\n======== Iter 1  ========\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTrain loss:  0.8481592326097085\nEval loss:  0.609836757183075\n\n======== Epoch 2 / 5 ========\nTrain loss:  0.6841135679836005\nEval loss:  0.8185145854949951\n\n======== Epoch 3 / 5 ========\nTrain loss:  0.6561007848088171\nEval loss:  0.47506988048553467\n\n======== Epoch 4 / 5 ========\nTrain loss:  0.6438469857397214\nEval loss:  0.4910270571708679\n\n======== Epoch 5 / 5 ========\nTrain loss:  0.6350655106591506\nEval loss:  0.6351885795593262\nIter eval loss:  0.6059273719787598\n======== Iter 2  ========\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTrain loss:  0.8555132091884882\nEval loss:  0.6127740144729614\n\n======== Epoch 2 / 5 ========\nTrain loss:  0.6942685615848487\nEval loss:  0.668502926826477\n\n======== Epoch 3 / 5 ========\nTrain loss:  0.6664743012105915\nEval loss:  0.4267936050891876\n\n======== Epoch 4 / 5 ========\nTrain loss:  0.6447444251725372\nEval loss:  0.5891205072402954\n\n======== Epoch 5 / 5 ========\nTrain loss:  0.6437784920276051\nEval loss:  0.8199254870414734\nIter eval loss:  0.623423308134079\n======== Iter 3  ========\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTrain loss:  0.8159941082269373\nEval loss:  0.6420449018478394\n\n======== Epoch 2 / 5 ========\nTrain loss:  0.6784639320742916\nEval loss:  0.6168662905693054\n\n======== Epoch 3 / 5 ========\nTrain loss:  0.6641563386984275\nEval loss:  0.5887134075164795\n\n======== Epoch 4 / 5 ========\nTrain loss:  0.6391049442996442\nEval loss:  0.6296753287315369\n\n======== Epoch 5 / 5 ========\nTrain loss:  0.6337927932470617\nEval loss:  0.724498450756073\nIter eval loss:  0.6403596758842468\n======== Iter 4  ========\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceRegression: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceRegression were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTrain loss:  0.8602627331102398\nEval loss:  0.5353209972381592\n\n======== Epoch 2 / 5 ========\nTrain loss:  0.6843631552978301\nEval loss:  0.5619345307350159\n\n======== Epoch 3 / 5 ========\nTrain loss:  0.6505449896966907\nEval loss:  0.5930908918380737\n\n======== Epoch 4 / 5 ========\nTrain loss:  0.6484680049855944\nEval loss:  0.6828595995903015\n\n======== Epoch 5 / 5 ========\nTrain loss:  0.6357929249044875\nEval loss:  0.8085628151893616\nIter eval loss:  0.6363537669181824\n0.6485959160327912\n","output_type":"stream"}]},{"cell_type":"code","source":"n_splits = 5\nn_repeats = 2\ngroup_count = 10\ncv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n                           group_count = group_count, random_state = 0, strategy = 'quantile')\n\nfor train_index, test_index in cv.split(input_ids, targets):\n    train_inputs, test_inputs = input_ids[train_index], input_ids[test_index]\n    train_targets, test_targets = targets[train_index], targets[test_index]\n    train_masks, test_masks = attention_masks[train_index], attention_masks[test_index]\n    train_type_ids, test_type_ids = token_type_ids[train_index], token_type_ids[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:05:14.636729Z","iopub.execute_input":"2021-06-18T12:05:14.637086Z","iopub.status.idle":"2021-06-18T12:05:14.680963Z","shell.execute_reply.started":"2021-06-18T12:05:14.637046Z","shell.execute_reply":"2021-06-18T12:05:14.680099Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"train_inputs = torch.tensor(train_inputs, dtype=torch.long)\ntest_inputs = torch.tensor(test_inputs, dtype=torch.long)\ntrain_targets = torch.tensor(train_targets, dtype=torch.float)\ntest_targets = torch.tensor(test_targets, dtype=torch.float)\ntrain_masks = torch.tensor(train_masks, dtype=torch.long)\ntest_masks = torch.tensor(test_masks, dtype=torch.long)\ntrain_type_ids = torch.tensor(train_type_ids, dtype=torch.long)\ntest_type_ids = torch.tensor(test_type_ids, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:05:14.682286Z","iopub.execute_input":"2021-06-18T12:05:14.682778Z","iopub.status.idle":"2021-06-18T12:05:14.692516Z","shell.execute_reply.started":"2021-06-18T12:05:14.682739Z","shell.execute_reply":"2021-06-18T12:05:14.691638Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(train_inputs, train_masks, train_type_ids, train_targets)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_type_ids, test_targets)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:05:14.693698Z","iopub.execute_input":"2021-06-18T12:05:14.694273Z","iopub.status.idle":"2021-06-18T12:05:14.701577Z","shell.execute_reply.started":"2021-06-18T12:05:14.694224Z","shell.execute_reply":"2021-06-18T12:05:14.700721Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"set_trainable(model, True)\nset_trainable(model.bert.embeddings, True)    \nset_trainable(model.bert.encoder, True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:05:14.702773Z","iopub.execute_input":"2021-06-18T12:05:14.703129Z","iopub.status.idle":"2021-06-18T12:05:14.738341Z","shell.execute_reply.started":"2021-06-18T12:05:14.703092Z","shell.execute_reply":"2021-06-18T12:05:14.737544Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"epochs = 5\noptimizer = AdamW(model.parameters(),\n                  lr = 1e-6,\n                  eps = 1e-6 \n                )\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\neval_losses = []\nfor epoch_i in range(0, epochs):\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    # training\n    model.train()\n    tr_loss = []\n    \n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        ids, input_mask, type_ids, target = batch\n        output = model(ids, input_mask, type_ids, target)\n        loss = RMSELoss(output, target)\n        tr_loss.append(loss.cpu().detach().numpy().tolist())\n        loss.backward()  \n        optimizer.step()\n        optimizer.zero_grad()  \n        scheduler.step()\n            \n    train_losses = np.mean(tr_loss)  \n    print(\"Train loss: \", train_losses)\n    all_targets, all_preds = [], []\n    model.eval()   \n    eval_loss = []\n    # evaluation\n    # disable gradients \n    with torch.no_grad(): \n        for batch in test_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n            ids, input_mask, type_ids, target = batch\n            output = model(ids, input_mask, type_ids, target)\n            loss = RMSELoss(output, target)\n        eval_loss.append(loss.cpu().detach().numpy().tolist())\n            \n    epoch_eval_loss = np.mean(eval_loss)\n    print(\"Eval loss: \", epoch_eval_loss)\n\n    eval_losses.append(epoch_eval_loss)   \ntorch.cuda.empty_cache()\nmean_eval_loss = np.mean(eval_losses)\nprint(mean_eval_loss)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:53:41.349965Z","iopub.execute_input":"2021-06-18T12:53:41.350294Z","iopub.status.idle":"2021-06-18T13:00:30.904937Z","shell.execute_reply.started":"2021-06-18T12:53:41.350262Z","shell.execute_reply":"2021-06-18T13:00:30.904117Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 7 ========\nTrain loss:  0.5632944694707092\nEval loss:  0.5638938546180725\n\n======== Epoch 2 / 7 ========\nTrain loss:  0.5484871620863256\nEval loss:  0.5631497502326965\n\n======== Epoch 3 / 7 ========\nTrain loss:  0.5405800640583038\nEval loss:  0.6666775345802307\n\n======== Epoch 4 / 7 ========\nTrain loss:  0.5323134526400499\nEval loss:  0.6047433614730835\n\n======== Epoch 5 / 7 ========\nTrain loss:  0.5210321541403381\nEval loss:  0.5654462575912476\n\n======== Epoch 6 / 7 ========\nTrain loss:  0.527106869388634\nEval loss:  0.6008443236351013\n\n======== Epoch 7 / 7 ========\nTrain loss:  0.5125665102206486\nEval loss:  0.6662572026252747\n0.6044303263936724\n","output_type":"stream"}]},{"cell_type":"code","source":"test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.281004Z","iopub.execute_input":"2021-06-18T12:10:07.281358Z","iopub.status.idle":"2021-06-18T12:10:07.292858Z","shell.execute_reply.started":"2021-06-18T12:10:07.281320Z","shell.execute_reply":"2021-06-18T12:10:07.292038Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"test[\"text\"] = test[\"excerpt\"].apply(lambda x: text_preprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.293979Z","iopub.execute_input":"2021-06-18T12:10:07.294354Z","iopub.status.idle":"2021-06-18T12:10:07.309957Z","shell.execute_reply.started":"2021-06-18T12:10:07.294313Z","shell.execute_reply":"2021-06-18T12:10:07.309165Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"excerpts = test.text.values","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.311033Z","iopub.execute_input":"2021-06-18T12:10:07.311391Z","iopub.status.idle":"2021-06-18T12:10:07.316210Z","shell.execute_reply.started":"2021-06-18T12:10:07.311351Z","shell.execute_reply":"2021-06-18T12:10:07.315280Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# convert sentences into tokens\ninput_ids = [tokenizer.encode(excerpt, add_special_tokens = True, max_length = MAX_LENGTH,\n                              padding='max_length') for excerpt in excerpts]\n\ninput_ids = np.array(input_ids)\nattention_masks = []\n# create a mask of 1 for all input tokens and 0 for all padding tokens\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]\nattention_masks = np.array(attention_masks)\n# create token type ids\ntoken_type_ids = [[0 for i in seq] for seq in input_ids]\ntoken_type_ids = np.array(token_type_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.317736Z","iopub.execute_input":"2021-06-18T12:10:07.318068Z","iopub.status.idle":"2021-06-18T12:10:07.351737Z","shell.execute_reply.started":"2021-06-18T12:10:07.318036Z","shell.execute_reply":"2021-06-18T12:10:07.350955Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"input_ids = torch.tensor(input_ids, dtype=torch.long)\nattention_masks = torch.tensor(attention_masks, dtype=torch.long)\ntoken_type_ids = torch.tensor(token_type_ids, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.353004Z","iopub.execute_input":"2021-06-18T12:10:07.353470Z","iopub.status.idle":"2021-06-18T12:10:07.359106Z","shell.execute_reply.started":"2021-06-18T12:10:07.353434Z","shell.execute_reply":"2021-06-18T12:10:07.358113Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"prediction_data = TensorDataset(input_ids, attention_masks, token_type_ids)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.360563Z","iopub.execute_input":"2021-06-18T12:10:07.360898Z","iopub.status.idle":"2021-06-18T12:10:07.367845Z","shell.execute_reply.started":"2021-06-18T12:10:07.360865Z","shell.execute_reply":"2021-06-18T12:10:07.367060Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"model.eval()   \npredictions , true_labels = [], []\n# evaluation\nfor batch in prediction_dataloader:\n    # disable gradients \n    batch = tuple(t.to(device) for t in batch)\n    ids, input_mask, type_ids = batch\n    with torch.no_grad():    \n        output = model(ids, input_mask, type_ids) \n    output = output.cpu().detach().numpy().tolist()\n    predictions += output","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.368957Z","iopub.execute_input":"2021-06-18T12:10:07.369340Z","iopub.status.idle":"2021-06-18T12:10:07.438228Z","shell.execute_reply.started":"2021-06-18T12:10:07.369308Z","shell.execute_reply":"2021-06-18T12:10:07.437517Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':test['id'],'target':predictions})","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.439921Z","iopub.execute_input":"2021-06-18T12:10:07.440387Z","iopub.status.idle":"2021-06-18T12:10:07.444975Z","shell.execute_reply.started":"2021-06-18T12:10:07.440354Z","shell.execute_reply":"2021-06-18T12:10:07.444303Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.448163Z","iopub.execute_input":"2021-06-18T12:10:07.448415Z","iopub.status.idle":"2021-06-18T12:10:07.462565Z","shell.execute_reply.started":"2021-06-18T12:10:07.448393Z","shell.execute_reply":"2021-06-18T12:10:07.461744Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:10:07.463852Z","iopub.execute_input":"2021-06-18T12:10:07.464391Z","iopub.status.idle":"2021-06-18T12:10:07.472992Z","shell.execute_reply.started":"2021-06-18T12:10:07.464358Z","shell.execute_reply":"2021-06-18T12:10:07.472191Z"},"trusted":true},"execution_count":123,"outputs":[]}]}