{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053d3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/annazhukovets/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## for data\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "import string \n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "## for sentiment\n",
    "from textblob import TextBlob\n",
    "## for ner, pos\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nltk.download('wordnet')\n",
    "## parameters searching\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "## rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14e7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "# a function  to create and save logs in the log files\n",
    "def log(path, file):\n",
    "    \"\"\"[Create a log file to record the experiment's logs]\n",
    "    \n",
    "    Arguments:\n",
    "        path {string} -- path to the directory\n",
    "        file {string} -- file name\n",
    "    \n",
    "    Returns:\n",
    "        [obj] -- [logger that record logs]\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the file exist\n",
    "    log_file = os.path.join(path, file)\n",
    "\n",
    "    if not os.path.isfile(log_file):\n",
    "        open(log_file, \"w+\").close()\n",
    "\n",
    "    console_logging_format = \"%(levelname)s %(message)s\"\n",
    "    file_logging_format = \"%(levelname)s: %(asctime)s: %(message)s\"\n",
    "\n",
    "    # configure logger\n",
    "    logging.basicConfig(level=logging.INFO, format=console_logging_format)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # create a file handler for output file\n",
    "    handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # set the logging level for log file\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # create a logging format\n",
    "    formatter = logging.Formatter(file_logging_format)\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e2f753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2834 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2829  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2830  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2832  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2833  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2829  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2830  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2831  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2832  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2833  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2829  1.711390        0.646900  \n",
       "2830  0.189476        0.535648  \n",
       "2831  0.255209        0.483866  \n",
       "2832 -0.215279        0.514128  \n",
       "2833  0.300779        0.512379  \n",
       "\n",
       "[2834 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('comlit_data/train.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad65db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['target'], axis=1)\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4282b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "class regressor_stratified_cv:\n",
    "    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n",
    "                 random_state = 0, strategy = 'quantile'):\n",
    "        self.group_count = group_count\n",
    "        self.strategy = strategy\n",
    "        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n",
    "                             random_state = random_state)\n",
    "        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n",
    "        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n",
    "                                            strategy = self.strategy)  \n",
    "            \n",
    "    def split(self, X, y, groups = None):\n",
    "        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n",
    "        return self.cv.split(X, kgroups, groups)\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups = None):\n",
    "        return self.cv.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8887dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "# n_repeats = 2\n",
    "# group_count = 10\n",
    "# cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "#                            group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "\n",
    "# for train_index, test_index in cv.split(X, y):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a875615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# pyplot.hist(y_train, bins = 10, alpha=0.5, label='train')\n",
    "# pyplot.hist(y_test, bins = 10, alpha=0.5, label='test');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a4b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# define the class FeatureGenerator\n",
    "# to generate new features\n",
    "class FeatureGenerator(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        ner = spacy.load(\"en_core_web_lg\")\n",
    "        # utils function to count the element of a list\n",
    "        def utils_lst_count(lst):\n",
    "            dic_counter = collections.Counter()\n",
    "            for x in lst:\n",
    "                dic_counter[x] += 1\n",
    "            dic_counter = collections.OrderedDict(\n",
    "                       sorted(dic_counter.items(),\n",
    "                       key=lambda x: x[1], reverse=True))\n",
    "            lst_count = [ {key:value} for key,value in dic_counter.items() ]\n",
    "            return lst_count\n",
    "        \n",
    "        # utils function create new column for each tag category\n",
    "        def utils_new_features(lst_dics_tuples, tag):\n",
    "            if len(lst_dics_tuples) > 0:\n",
    "                tag_type = []\n",
    "                for dic_tuples in lst_dics_tuples:\n",
    "                    for tuple in dic_tuples:\n",
    "                        type, n = tuple[1], dic_tuples[tuple]\n",
    "                        tag_type = tag_type + [type]*n\n",
    "                        dic_counter = collections.Counter()\n",
    "                        for x in tag_type:\n",
    "                            dic_counter[x] += 1\n",
    "                return dic_counter[tag]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "            \n",
    "        # num of words in excerpt\n",
    "        x_dataset['word_count'] = x_dataset[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "        # num of chars in excerpt\n",
    "        x_dataset['char_count'] = x_dataset[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "        # num of sentences in excerpt\n",
    "        x_dataset['sentence_count'] = x_dataset[\"excerpt\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "        # avg word len in excerpt\n",
    "        x_dataset['avg_word_length'] = x_dataset['char_count'] / x_dataset['word_count']\n",
    "        # avg sentence len in excerpt\n",
    "        x_dataset['avg_sentence_lenght'] = x_dataset['word_count'] / x_dataset['sentence_count']\n",
    "        # sentiment index of excerpt\n",
    "        x_dataset[\"sentiment\"] = x_dataset[\"excerpt\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "        # preprocessing for nlp\n",
    "        x_dataset[\"nlp_text\"] = x_dataset[\"excerpt\"].apply(lambda x: nlp(x))\n",
    "        # ner tag text and exctract tags into a list\n",
    "        x_dataset[\"ner_tags\"] = x_dataset[\"nlp_text\"].apply(lambda x: [(tag.text, tag.label_) \n",
    "                                for tag in x.ents] )\n",
    "        # count ner tags\n",
    "        x_dataset[\"ner_tags\"] = x_dataset[\"ner_tags\"].apply(lambda x: utils_lst_count(x))\n",
    "        # extract ner features\n",
    "        ner_tags_set = ['QUANTITY', 'MONEY', 'GPE',\n",
    "                    'NORP', 'CARDINAL', 'LOC',\n",
    "                    'ORDINAL', 'PRODUCT', 'FAC',\n",
    "                    'LANGUAGE', 'TIME', 'LAW',\n",
    "                    'EVENT', 'ORG', 'PERCENT',\n",
    "                    'WORK_OF_ART', 'PERSON', 'DATE']\n",
    "        for feature in ner_tags_set:\n",
    "            x_dataset[\"ner_tags_\" + feature] = x_dataset[\"ner_tags\"].apply(lambda x:\n",
    "                                                                 utils_new_features(x, feature))\n",
    "        \n",
    "        # pos tag text and exctract tags into a list\n",
    "        x_dataset[\"pos_tags\"] = x_dataset[\"nlp_text\"].apply(lambda x: [(token.text, token.tag_) \n",
    "                                for token in x] )\n",
    "        # count pos tags\n",
    "        x_dataset[\"pos_tags\"] = x_dataset[\"pos_tags\"].apply(lambda x: utils_lst_count(x))\n",
    "        # extract pos features\n",
    "        pos_tags_set = ['CC', 'POS', 'WDT', 'VBP', 'FW', ':', 'PRP$',\n",
    "                    'WRB', 'PRP', 'RP', 'RBS', 'NNP', 'CD', 'EX', 'PDT',\n",
    "                    'VBN', 'WP$', 'JJ', 'SYM', 'VBG', 'VB', 'JJS', 'VBD',\n",
    "                    'WP', ',', 'NNS', 'NN', 'VBZ', 'MD', 'RB', 'DT',\n",
    "                    'JJR', 'UH', 'NNPS', 'TO', 'RBR']\n",
    "    \n",
    "        for feature in pos_tags_set:\n",
    "            x_dataset[\"pos_tags_\" + feature] = x_dataset[\"pos_tags\"].apply(lambda x:\n",
    "                                                                 utils_new_features(x, feature))\n",
    "    \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7599b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class IDFVectorizer\n",
    "# to generate new feature with mean of idf\n",
    "class IDFVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        # removal of punctuation\n",
    "        PUNCT_TO_REMOVE = string.punctuation\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "        \n",
    "        # removal of stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        def remove_stopwords(text):\n",
    "            return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "        \n",
    "        # lemmatization \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatize_words(text):\n",
    "            return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        \n",
    "        # word frequency in docs\n",
    "        def doc_freq(word):\n",
    "            c = 0\n",
    "            try:\n",
    "                c = DF[word]\n",
    "            except:\n",
    "                pass\n",
    "            return c\n",
    "        \n",
    "        # idf vector generation\n",
    "        def mean_of_vector(tokens):\n",
    "            idf_vec = []\n",
    "            for token in np.unique(tokens):\n",
    "                df = doc_freq(token)\n",
    "                idf = np.log(N/(df + 1))\n",
    "                try:\n",
    "                    idf_vec.append(idf)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return np.mean(idf_vec)\n",
    "        \n",
    "        # lower casing\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt\"].str.lower()\n",
    "        # removal of punctuation\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_punctuation(text))\n",
    "        # removal of stopwords\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_stopwords(text))\n",
    "        # lemmatization \n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: lemmatize_words(text))    \n",
    "        # tokenizetion\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda x: [token for token in word_tokenize(x)])\n",
    "        \n",
    "        N = len(x_dataset[\"excerpt\"])\n",
    "        DF = {}\n",
    "        for i in range(N):\n",
    "            tokens = x_dataset[\"excerpt_proc\"].iloc[i]\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "            \n",
    "\n",
    "        for i in DF:\n",
    "            DF[i] = len(DF[i]) \n",
    "\n",
    "        x_dataset['idf_vec'] = x_dataset[\"excerpt_proc\"].apply(lambda x:  mean_of_vector(x))\n",
    "        \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4274bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing step\n",
    "# Drop the columns \n",
    "from sklearn.compose import ColumnTransformer\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['id', \n",
    "                                                                        'url_legal', \n",
    "                                                                        'license',\n",
    "                                                                        'excerpt',\n",
    "                                                                        'standard_error',\n",
    "                                                                        'ner_tags',\n",
    "                                                                        'pos_tags',\n",
    "                                                                        'excerpt_proc',\n",
    "                                                                        'nlp_text'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f202e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('get_new_columns', FeatureGenerator())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('get_new_columns', FeatureGenerator()),\n",
    "#                  ('idf_vect', IDFVectorizer()),\n",
    "#                  ('pre_processing',pre_process),\n",
    "#                  ('random_forest', RandomForestRegressor(bootstrap = True, max_depth = 40, \n",
    "#                                                         max_features = 'auto', min_samples_split = 10,\n",
    "#                                                         min_samples_leaf = 4, n_estimators = 800))\n",
    "                ])\n",
    "\n",
    "# fit the pipeline with the training data\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92125a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6287fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "# # test\n",
    "# with open('pickles/test.pk', 'wb') as output:\n",
    "#     pickle.dump(test, output)\n",
    "\n",
    "# X\n",
    "with open('pickles/X.pk', 'wb') as output:\n",
    "    pickle.dump(X, output)\n",
    "\n",
    "# y\n",
    "with open('pickles/y.pk', 'wb') as output:\n",
    "    pickle.dump(y, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6136ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
