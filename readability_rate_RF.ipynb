{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053d3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/annazhukovets/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## for data\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "import string \n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "## for sentiment\n",
    "from textblob import TextBlob\n",
    "## for ner, pos\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nltk.download('wordnet')\n",
    "## parameters searching\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "## rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14e7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "# a function  to create and save logs in the log files\n",
    "def log(path, file):\n",
    "    \"\"\"[Create a log file to record the experiment's logs]\n",
    "    \n",
    "    Arguments:\n",
    "        path {string} -- path to the directory\n",
    "        file {string} -- file name\n",
    "    \n",
    "    Returns:\n",
    "        [obj] -- [logger that record logs]\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the file exist\n",
    "    log_file = os.path.join(path, file)\n",
    "\n",
    "    if not os.path.isfile(log_file):\n",
    "        open(log_file, \"w+\").close()\n",
    "\n",
    "    console_logging_format = \"%(levelname)s %(message)s\"\n",
    "    file_logging_format = \"%(levelname)s: %(asctime)s: %(message)s\"\n",
    "\n",
    "    # configure logger\n",
    "    logging.basicConfig(level=logging.INFO, format=console_logging_format)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # create a file handler for output file\n",
    "    handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # set the logging level for log file\n",
    "    handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # create a logging format\n",
    "    formatter = logging.Formatter(file_logging_format)\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad65db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "with open('pickles/X.pk', 'rb') as data:\n",
    "    X = pickle.load(data)\n",
    "\n",
    "# y\n",
    "with open('pickles/y.pk', 'rb') as data:\n",
    "    y = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4282b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "class regressor_stratified_cv:\n",
    "    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n",
    "                 random_state = 0, strategy = 'quantile'):\n",
    "        self.group_count = group_count\n",
    "        self.strategy = strategy\n",
    "        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n",
    "                             random_state = random_state)\n",
    "        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n",
    "        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n",
    "                                            strategy = self.strategy)  \n",
    "            \n",
    "    def split(self, X, y, groups = None):\n",
    "        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n",
    "        return self.cv.split(X, kgroups, groups)\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups = None):\n",
    "        return self.cv.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8887dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 5\n",
    "# n_repeats = 2\n",
    "# group_count = 10\n",
    "# cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "#                            group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "\n",
    "# for train_index, test_index in cv.split(X, y):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a875615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# pyplot.hist(y_train, bins = 10, alpha=0.5, label='train')\n",
    "# pyplot.hist(y_test, bins = 10, alpha=0.5, label='test');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7599b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class IDFVectorizer\n",
    "# to generate new feature with mean of idf\n",
    "class IDFVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_dataset, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        # removal of punctuation\n",
    "        PUNCT_TO_REMOVE = string.punctuation\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "        \n",
    "        # removal of stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        def remove_stopwords(text):\n",
    "            return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "        \n",
    "        # lemmatization \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatize_words(text):\n",
    "            return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        \n",
    "        # word frequency in docs\n",
    "        def doc_freq(word):\n",
    "            c = 0\n",
    "            try:\n",
    "                c = DF[word]\n",
    "            except:\n",
    "                pass\n",
    "            return c\n",
    "        \n",
    "        # idf vector generation\n",
    "        def mean_of_vector(tokens):\n",
    "            idf_vec = []\n",
    "            for token in np.unique(tokens):\n",
    "                df = doc_freq(token)\n",
    "                idf = np.log(N/(df + 1))\n",
    "                try:\n",
    "                    idf_vec.append(idf)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return np.mean(idf_vec)\n",
    "        \n",
    "        # lower casing\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt\"].str.lower()\n",
    "        # removal of punctuation\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_punctuation(text))\n",
    "        # removal of stopwords\n",
    "#         x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_stopwords(text))\n",
    "        # lemmatization \n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: lemmatize_words(text))    \n",
    "        # tokenizetion\n",
    "        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda x: [token for token in word_tokenize(x)])\n",
    "        \n",
    "        N = len(x_dataset[\"excerpt\"])\n",
    "        DF = {}\n",
    "        for i in range(N):\n",
    "            tokens = x_dataset[\"excerpt_proc\"].iloc[i]\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "            \n",
    "\n",
    "        for i in DF:\n",
    "            DF[i] = len(DF[i]) \n",
    "\n",
    "        x_dataset['idf_vec'] = x_dataset[\"excerpt_proc\"].apply(lambda x:  mean_of_vector(x))\n",
    "        \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18a3729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log(path=\"logs/\", file=\"logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6a9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing step\n",
    "# Drop the columns \n",
    "from sklearn.compose import ColumnTransformer\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['id', \n",
    "                                                                        'url_legal', \n",
    "                                                                        'license',\n",
    "                                                                        'excerpt',\n",
    "                                                                        'standard_error',\n",
    "                                                                        'ner_tags',\n",
    "                                                                        'pos_tags',\n",
    "                                                                        'excerpt_proc',\n",
    "                                                                        'nlp_text'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f202e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([#('get_new_columns', FeatureGenerator()),\n",
    "                 ('idf_vect', IDFVectorizer()),\n",
    "                 ('pre_processing',pre_process),\n",
    "                 ('random_forest', RandomForestRegressor(bootstrap = True, max_depth = 50, \n",
    "                                                         max_features = 'auto', min_samples_leaf = 4, \n",
    "                                                         min_samples_split = 10, n_estimators = 800))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef8f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Train RF with stopwords\n",
      "INFO The rmse for RF iteration 0: 0.823\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 1: 0.836\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 2: 0.792\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 3: 0.782\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 4: 0.832\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 5: 0.817\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 6: 0.820\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 7: 0.793\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 8: 0.811\n",
      "INFO -------------------------------\n",
      "INFO The rmse for RF iteration 9: 0.826\n",
      "INFO -------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "n_repeats = 2\n",
    "group_count = 10\n",
    "cv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n",
    "                           group_count = group_count, random_state = 0, strategy = 'quantile')\n",
    "\n",
    "\n",
    "logger.info(\"Train RF with stopwords\")\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predict = pipe.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, predict, squared=False)\n",
    "    logger.info(\"The rmse for RF iteration {}: {:.3f}\".format(i, rmse))\n",
    "    logger.info(\"-------------------------------\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae224454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.02088\n",
      "Feature: 1, Score: 0.02848\n",
      "Feature: 2, Score: 0.00618\n",
      "Feature: 3, Score: 0.02018\n",
      "Feature: 4, Score: 0.08416\n",
      "Feature: 5, Score: 0.01921\n",
      "Feature: 6, Score: 0.00077\n",
      "Feature: 7, Score: 0.00010\n",
      "Feature: 8, Score: 0.00591\n",
      "Feature: 9, Score: 0.00146\n",
      "Feature: 10, Score: 0.00500\n",
      "Feature: 11, Score: 0.00223\n",
      "Feature: 12, Score: 0.00434\n",
      "Feature: 13, Score: 0.00012\n",
      "Feature: 14, Score: 0.00023\n",
      "Feature: 15, Score: 0.00007\n",
      "Feature: 16, Score: 0.00184\n",
      "Feature: 17, Score: 0.00004\n",
      "Feature: 18, Score: 0.00164\n",
      "Feature: 19, Score: 0.00363\n",
      "Feature: 20, Score: 0.00001\n",
      "Feature: 21, Score: 0.00032\n",
      "Feature: 22, Score: 0.00877\n",
      "Feature: 23, Score: 0.00660\n",
      "Feature: 24, Score: 0.01329\n",
      "Feature: 25, Score: 0.00316\n",
      "Feature: 26, Score: 0.00696\n",
      "Feature: 27, Score: 0.01008\n",
      "Feature: 28, Score: 0.00006\n",
      "Feature: 29, Score: 0.01450\n",
      "Feature: 30, Score: 0.00879\n",
      "Feature: 31, Score: 0.00475\n",
      "Feature: 32, Score: 0.01095\n",
      "Feature: 33, Score: 0.00585\n",
      "Feature: 34, Score: 0.00125\n",
      "Feature: 35, Score: 0.01379\n",
      "Feature: 36, Score: 0.00848\n",
      "Feature: 37, Score: 0.00275\n",
      "Feature: 38, Score: 0.00261\n",
      "Feature: 39, Score: 0.02653\n",
      "Feature: 40, Score: 0.00020\n",
      "Feature: 41, Score: 0.01167\n",
      "Feature: 42, Score: 0.00004\n",
      "Feature: 43, Score: 0.01400\n",
      "Feature: 44, Score: 0.00970\n",
      "Feature: 45, Score: 0.00270\n",
      "Feature: 46, Score: 0.04166\n",
      "Feature: 47, Score: 0.00356\n",
      "Feature: 48, Score: 0.01242\n",
      "Feature: 49, Score: 0.02517\n",
      "Feature: 50, Score: 0.01653\n",
      "Feature: 51, Score: 0.01647\n",
      "Feature: 52, Score: 0.00844\n",
      "Feature: 53, Score: 0.01472\n",
      "Feature: 54, Score: 0.02003\n",
      "Feature: 55, Score: 0.00403\n",
      "Feature: 56, Score: 0.00160\n",
      "Feature: 57, Score: 0.00138\n",
      "Feature: 58, Score: 0.00794\n",
      "Feature: 59, Score: 0.00198\n",
      "Feature: 60, Score: 0.42973\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get importance\n",
    "importance = pipe.steps[2][1].feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e891997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto', 'sqrt']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "# max_depth.append(None)\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [2, 5, 10]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'random_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=pipe,\n",
    "#                                    param_distributions=random_grid,\n",
    "#                                    n_iter=5,\n",
    "#                                    scoring='neg_mean_absolute_error',\n",
    "#                                    cv=3, \n",
    "#                                    verbose=1, \n",
    "#                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bdc9ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('idf_vect', IDFVectorizer()),\n",
       "                                             ('pre_processing',\n",
       "                                              ColumnTransformer(remainder='passthrough',\n",
       "                                                                transformers=[('drop_columns',\n",
       "                                                                               'drop',\n",
       "                                                                               ['id',\n",
       "                                                                                'url_legal',\n",
       "                                                                                'license',\n",
       "                                                                                'excerpt',\n",
       "                                                                                'standard_error',\n",
       "                                                                                'ner_tags',\n",
       "                                                                                'pos_tags',\n",
       "                                                                                'excerpt_proc',\n",
       "                                                                                'nlp_text'])])),\n",
       "                                             ('random_forest',\n",
       "                                              RandomForestRegressor())]),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'random_forest__bootstrap': [True,\n",
       "                                                                     False],\n",
       "                                        'random_forest__max_depth': [20, 40, 60,\n",
       "                                                                     80, 100,\n",
       "                                                                     None],\n",
       "                                        'random_forest__max_features': ['auto',\n",
       "                                                                        'sqrt'],\n",
       "                                        'random_forest__min_samples_leaf': [1,\n",
       "                                                                            2,\n",
       "                                                                            4],\n",
       "                                        'random_forest__min_samples_split': [2,\n",
       "                                                                             5,\n",
       "                                                                             10],\n",
       "                                        'random_forest__n_estimators': [200,\n",
       "                                                                        400,\n",
       "                                                                        600,\n",
       "                                                                        800,\n",
       "                                                                        1000]},\n",
       "                   random_state=8, scoring='neg_mean_absolute_error',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2525b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'random_forest__n_estimators': 800, 'random_forest__min_samples_split': 10, 'random_forest__min_samples_leaf': 4, 'random_forest__max_features': 'auto', 'random_forest__max_depth': 40, 'random_forest__bootstrap': True}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "-0.674254618935198\n"
     ]
    }
   ],
   "source": [
    "# print(\"The best hyperparameters from Random Search are:\")\n",
    "# print(random_search.best_params_)\n",
    "# print(\"\")\n",
    "# print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "# print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de913023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [800]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [30, 40, 50]\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [10, 20]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [4, 6]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True]\n",
    "\n",
    "# # Create the random grid\n",
    "# parameters = {'random_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62584692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(pipe, param_grid=parameters, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2b539cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 39.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('idf_vect', IDFVectorizer()),\n",
       "                                       ('pre_processing',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('drop_columns',\n",
       "                                                                         'drop',\n",
       "                                                                         ['id',\n",
       "                                                                          'url_legal',\n",
       "                                                                          'license',\n",
       "                                                                          'excerpt',\n",
       "                                                                          'standard_error',\n",
       "                                                                          'ner_tags',\n",
       "                                                                          'pos_tags',\n",
       "                                                                          'excerpt_proc',\n",
       "                                                                          'nlp_text'])])),\n",
       "                                       ('random_forest',\n",
       "                                        RandomForestRegressor())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'random_forest__bootstrap': [True],\n",
       "                         'random_forest__max_depth': [30, 40, 50],\n",
       "                         'random_forest__max_features': ['auto'],\n",
       "                         'random_forest__min_samples_leaf': [4, 6],\n",
       "                         'random_forest__min_samples_split': [10, 20],\n",
       "                         'random_forest__n_estimators': [800]},\n",
       "             scoring='neg_mean_absolute_error', verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76723e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'random_forest__bootstrap': True, 'random_forest__max_depth': 50, 'random_forest__max_features': 'auto', 'random_forest__min_samples_leaf': 4, 'random_forest__min_samples_split': 10, 'random_forest__n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # n_estimators\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# # max_features\n",
    "# max_features = ['auto', 'sqrt']\n",
    "\n",
    "# # max_depth\n",
    "# max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "# max_depth.append(None)\n",
    "\n",
    "# # min_samples_split\n",
    "# min_samples_split = [2, 5, 10]\n",
    "\n",
    "# # min_samples_leaf\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# # bootstrap\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'rrandom_forest__n_estimators': n_estimators,\n",
    "#                'random_forest__max_features': max_features,\n",
    "#                'random_forest__max_depth': max_depth,\n",
    "#                'random_forest__min_samples_split': min_samples_split,\n",
    "#                'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "#                'random_forest__bootstrap': bootstrap}\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=pipe,\n",
    "#                                    param_distributions=random_grid,\n",
    "#                                    n_iter=5,\n",
    "#                                    scoring='neg_root_mean_squared_error',\n",
    "#                                    cv=3, \n",
    "#                                    verbose=1, \n",
    "#                                    random_state=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
